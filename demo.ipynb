{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional colab/kaggle setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def download_data():\n",
    "    !git clone https://github.com/tobihol/aapor-finetuning.git\n",
    "    %mv aapor-finetuning/data/ .\n",
    "    %rm -rf aapor-finetuning/\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def is_running_in_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "def is_running_in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "if is_running_in_colab() or is_running_in_kaggle():\n",
    "    print(\"Running on Colab/Kaggle\")\n",
    "    download_data()\n",
    "else:\n",
    "    print(\"Not running in Colab/Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Model Selection\n",
    "\n",
    "Think about your use case an pick a model. \n",
    "\n",
    "**Questions to consider:**\n",
    "\n",
    "- Do you want to run it on your own hardware?\n",
    "- Do you have funds to use for a proprietary model?\n",
    "- Do you have strong programming skills?\n",
    "\n",
    "**Example use-case:**\n",
    "\n",
    "- Predicting vote choice in the US presidential election based on ANES survey answer.\n",
    "\n",
    "**Resources**\n",
    "\n",
    "- The popular benchmark **Chatbot Arena** _(Chiang et al., 2024)_  provides a live updated elo score based on crowd-sourced pairwise comparisons of their responses.\n",
    "    - Trade-off Plot: https://lmarena.ai/price\n",
    "- The **Stanford HELM Benchmark** evaluates language models on over 50 tasks, including understanding and reasoning. It provides insights into model performance in terms of accuracy, fairness, and robustness.\n",
    "    - https://crfm.stanford.edu/helm/\n",
    "- The **Artificial Analysis** platform provides independent analysis of AI models and API providers. The main metrics are 'intelligence', speed, and price. \n",
    "    - https://artificialanalysis.ai/\n",
    "- The **Open LLM Leaderboard** on Hugging Face runs multiple well established benchmarks on for open-source models hosted on the site. It is most useful to compare smaller open-source LLMs that are not featured on Chatbot Arena or compare different fine-tunes of models. \n",
    "    - https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "Feel free to make notes here, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Excel to JSON\n",
    "\n",
    "What would the corresponding JSON entry look like?\n",
    "\n",
    "|   Age | Gender   | State     | Vote Choice   |\n",
    "|------:|:---------|:----------|:--------------|\n",
    "|    27 | female   | Louisiana | Trump         |\n",
    "|    45 | male     | nan       | Clinton       |\n",
    "|   nan | female   | Ohio      | Non-voter     |\n",
    "\n",
    "\n",
    "**Example**\n",
    "\n",
    "Excel\n",
    "|   Age | Gender | Education | Occupation |\n",
    "|------:|:-------|:----------|:-----------|\n",
    "|    30 | male   | bachelor  | accountant |\n",
    "|    24 | female | master    | engineer   |\n",
    "\n",
    "JSON\n",
    "```json\n",
    "{\n",
    "    \"participants\": [\n",
    "        {\n",
    "            \"age\": 30,\n",
    "            \"gender\": \"male\",\n",
    "            \"education\": \"bachelor\",\n",
    "            \"occupation\": \"accountant\"\n",
    "        },\n",
    "        {\n",
    "            \"age\": 24,\n",
    "            \"gender\": \"female\",\n",
    "            \"education\": \"master\",\n",
    "            \"occupation\": \"engineer\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Extra Question:**\n",
    "Are there different ways to construct the JSON?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type up your answer here\n",
    "\n",
    "json_data = {\n",
    "    \"participants\": [\n",
    "        # TODO\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice this would be done automatically,\n",
    "# here is are two different examples of how to do so\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"data/json_task.xlsx\", index_col=0)\n",
    "\n",
    "print(\"--- Records Orientation ---\")\n",
    "print(data.to_json(orient=\"records\"))\n",
    "print()\n",
    "\n",
    "print(\"--- Columns Orientation ---\")\n",
    "print(data.to_json(orient=\"columns\"))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: JSON Prompt\n",
    "\n",
    "Build a prompt for your task.\n",
    "\n",
    "**Questions to consider:**\n",
    "- What should the output look like?\n",
    "- Should the model act as an expert in a certain field? Who does it substitute?\n",
    "\n",
    "\n",
    "**Example use-case:**\n",
    "- Predicting vote choice in the US presidential election based on ANES survey answer\n",
    "\n",
    "**Example Conversion**\n",
    "\n",
    "SURVEY DATA\n",
    "```json\n",
    "{\n",
    "    \"participants\": [\n",
    "        {\n",
    "            \"age\": 30,\n",
    "            \"gender\": \"male\",\n",
    "            \"education\": \"bachelor\",\n",
    "            \"occupation\": \"accountant\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "PROMPT\n",
    "```json\n",
    "{\n",
    "    \"prompt\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a survey participant. Reply to the user's question with a short answer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your age?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am 30 years old.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your gender?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am a male.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your occupation?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am an accountant.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type up your answer here\n",
    "\n",
    "json_prompt = {\n",
    "    \"prompt\": [\n",
    "        # TODO\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our prompt with a small open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# id of a model hosted on Hugging Face\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Create a pipeline for an instruct model using the json_prompt as input\n",
    "instruct_pipeline = pipeline(\"text-generation\", model=model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response using the instruct model pipeline\n",
    "responses = instruct_pipeline(json_prompt[\"prompt\"], num_return_sequences=10)\n",
    "\n",
    "# Print the generated response\n",
    "print(\"10 different generated responses:\")\n",
    "[resp[\"generated_text\"][-1] for resp in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Pipeline\n",
    "In the following we go through the steps required to setup a fine-tuning pipeline to impute missing survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional colab/kaggle setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_dependencies():\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "      print(\"CUDA is not available. \\nPick a GPU before running this notebook. \\nGo to 'Runtime' -> 'Change runtime type' to do this. (Colab)\")\n",
    "      return \n",
    "    %pip install transformers\n",
    "    %pip install datasets\n",
    "    %pip install peft\n",
    "    %pip install evaluate\n",
    "    %pip install wandb\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def is_running_in_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "def is_running_in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "if is_running_in_colab() or is_running_in_kaggle():\n",
    "    print(\"Running on Colab/Kaggle\")\n",
    "    install_dependencies()\n",
    "else:\n",
    "    print(\"Not running in Colab/Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "seed = 24 # Please set your own favorite seed!\n",
    "transformers.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation\n",
    "We use 2016 American National Election Studies survey data. Specifically, the subset of data Argyle et al. (2022) used in study 2 (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JPV20K)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey = pd.read_csv(\"data/2016_anes_argyle.csv\")\n",
    "df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null values and data types\n",
    "df_survey.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"race\",\n",
    "    \"discuss_politics\",\n",
    "    \"ideology\",\n",
    "    \"party\",\n",
    "    \"church_goer\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"political_interest\",\n",
    "    \"patriotism\",\n",
    "    \"state\",\n",
    "]\n",
    "label = \"ground_truth\" # this is the vote choice in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tread missing values as a category \n",
    "df_survey_processed = (\n",
    "    df_survey\n",
    "    .astype({\"age\": str})\n",
    "    .fillna(\"missing\")\n",
    ")\n",
    "df_survey_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split\n",
    "\n",
    "Any manipulation of the training data should be done in the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_survey_processed, test_size=0.2, random_state=seed)\n",
    "\n",
    "# we can modify the training data here to do different experiments\n",
    "# for example excluding republican voters\n",
    "# leans_republican = df_train[\"party\"].apply(lambda x: \"Republican\" in x)\n",
    "# df_train = df_train[~leans_republican]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(df_test, preserve_index=False),\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Design\n",
    "\n",
    "We will use an instruction-tuned model and will therefore define an instruction prompt here. Having distinct `user` and `assistant` text. Where the `user` prompt includes all conditioning of the model and the `assistant` text is the expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"Please perform a classification task. \"\n",
    "    \"Given the 2016 survey answers from the American National Election Studies, \"\n",
    "    \"return which candidate the person voted for. \"\n",
    "    \"Return a label from ['Trump', 'Clinton', 'Non-voter'] only without any other text.\\n\"\n",
    ")\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_map = {\n",
    "    \"race\": \"Race\",\n",
    "    \"discuss_politics\": \"Discusses politics\",\n",
    "    \"ideology\": \"Ideology\",\n",
    "    \"party\": \"Party\",\n",
    "    \"church_goer\": \"Church\",\n",
    "    \"age\": \"Age\",\n",
    "    \"gender\": \"Gender\",\n",
    "    \"political_interest\": \"Political interest\",\n",
    "    \"patriotism\": \"American Flag\",\n",
    "    \"state\": \"State\",\n",
    "    \"ground_truth\": \"Vote\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_prompt_completion(\n",
    "    row: dict,\n",
    "    system_prompt: str = instruction,\n",
    ") -> list[list[dict]]:\n",
    "    user_prompt = \"\\n\".join(\n",
    "        [f\"{column_name_map[k]}: {v}\" for k, v in row.items() if k != label]\n",
    "    )\n",
    "    assistant_prompt = row[label]\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            },\n",
    "        ],\n",
    "        \"completion\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "build_prompt_completion(\n",
    "    row=dataset[\"train\"][0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_llm = dataset.map(build_prompt_completion).remove_columns(features+[label])\n",
    "dataset_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "We use a very small (1B parameters) model here for demonstration proposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # revision=revision, # NOTE: revision should be set for an reproducible experiment \n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Quantization reduces the memory required to store the model (Dettmers et al., 2022). Typically a model is stored in 16-bit precision, therefore for a 70B parameter model:\n",
    "\n",
    "$$\\frac{16 \\text{ bits}}{8 \\text{ bits/byte}} \\times 70 \\times 10^9 \\text{ parameters} = 140 \\text{ GB of VRAM}$$\n",
    "\n",
    "With 4-bit quantization, all parameters are stored in 4-bit precision, reducing the memory requirement to:\n",
    "\n",
    "$$\\frac{4 \\text{ bits}}{8 \\text{ bits/byte}} \\times 70 \\times 10^9 \\text{ parameters} = 35 \\text{ GB of VRAM}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model in 4bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA\n",
    "Low-Rank Adapters (LoRA) are a parameter efficient fine-tuning method (Hu et al., 2021). Instead of finetuning all model weights, LoRA finetunes the weights of the adapter layers only. This requires less memory and allows for faster finetuning.\n",
    "\n",
    "![LoRA Diagram](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=\"all-linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl\n",
    "\n",
    "trl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "if getattr(model.config, \"pad_token_id\") is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "labels = df_survey_processed.ground_truth.unique()\n",
    "\n",
    "first_token_ids = [\n",
    "    tokenizer.encode(label, add_special_tokens=False)[0]\n",
    "    for label in labels\n",
    "]\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    # Return dist of relevant tokens\n",
    "    return logits[:, :, first_token_ids]\n",
    "\n",
    "y_probas = []\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    labels = labels[:, 1:]\n",
    "    logits = logits[:, :-1]\n",
    "\n",
    "    logits_first_token_dist = np.array(\n",
    "        [\n",
    "            [\n",
    "                logit_id\n",
    "                for logit_id, label_id in zip(logit_ids, label_ids)\n",
    "                if label_id != -100\n",
    "            ][0]\n",
    "            for logit_ids, label_ids in zip(logits, labels)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logits_first_token_dist_exp = np.exp(logits_first_token_dist)\n",
    "    normalized_logits_first_token_dist = (\n",
    "        logits_first_token_dist_exp\n",
    "        / logits_first_token_dist_exp.sum(axis=1, keepdims=True)\n",
    "    )\n",
    "\n",
    "    y_probas.append(normalized_logits_first_token_dist)\n",
    "\n",
    "    df_y_proba = pd.DataFrame(\n",
    "        normalized_logits_first_token_dist, columns=labels\n",
    "    )\n",
    "\n",
    "    wandb.log(\n",
    "        data={\n",
    "            \"y_proba\": wandb.Table(data=df_y_proba),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # -100 is a default value for ignore_index used by DataCollatorForCompletionOnlyLM\n",
    "    mask = labels == -100\n",
    "    labels[mask] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = list(df_y_proba.idxmax(axis=1))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_pred=decoded_preds, y_true=decoded_labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "To run the training without wandb logging, set `wandb.init(mode='disabled')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # batch size\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    # gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    # completion_only_loss=True,\n",
    "    # eval\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    # saving\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    # other\n",
    "    # max_seq_length=max_seq_length,\n",
    "    output_dir=\"./results\",\n",
    "    # num_train_epochs=self.n_epochs,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # data_collator=collator,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()  # for zero-shot evaluation\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fine-tuning results can be found live at: https://wandb.ai/tobihol/survai-finetuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
