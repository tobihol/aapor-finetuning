{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAPOR Short Course: Fine-tuning LLMs for Data Augmentation\n",
    "\n",
    "The following contains exercises 1, 2, and 3, and the fine-tuning pipeline we use during this short course.\n",
    "\n",
    "This Jupyter Notebook is designed to be worked from top to bottom. However, the code of each section can be run independently, i.e., you can run the notebook starting from the [Exercises](#Exercises) or the [Fine-tuning Pipeline](#Fine-tuning-Pipeline) section.\n",
    "\n",
    "To run some parts of the pipeline a GPU server is required. To select a GPU server on Google Colab, go to `Runtime` -> `Change runtime type` to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "The following contains the exercises 1, 2, and 3 of the short-course. The `TODO` marks places where the answer should be written/coded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "Before starting the exercises, some technical setup is required, when running the notebook on Google Colab or Kaggle. \n",
    "\n",
    "For the fine-tuning, we have additional technical setup later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the sys module to access system-specific parameters and functions\n",
    "import sys\n",
    "# Importing the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Downloads the data from the GitHub repository of this notebook\n",
    "def download_data():\n",
    "    !git clone https://github.com/tobihol/aapor-finetuning.git\n",
    "    %mv aapor-finetuning/data/ .\n",
    "    %rm -rf aapor-finetuning/\n",
    "    return\n",
    "\n",
    "# This function checks if CUDA (GPU support) is available and installs necessary dependencies for fine-tuning\n",
    "# If CUDA is not available, it warns the user to select a GPU runtime\n",
    "# The function installs the following packages:\n",
    "# - transformers: Hugging Face's library for working with pre-trained models\n",
    "# - openpyxl: Library for reading and writing Excel files (xlsx/xlsm/xltx/xltm)\n",
    "def install_dependencies():\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "      print(\"CUDA is not available. \\nPick a GPU before running this notebook. \\nGo to 'Runtime' -> 'Change runtime type' to do this. (Colab)\")\n",
    "      return \n",
    "    %pip install transformers\n",
    "    %pip install openpyxl\n",
    "    return\n",
    "\n",
    "# Checks if the code is running in a Kaggle environment\n",
    "def is_running_in_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "# Checks if the code is running in a Google Colab environment\n",
    "def is_running_in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "# Check if running in Colab or Kaggle and download data accordingly\n",
    "if is_running_in_colab() or is_running_in_kaggle():\n",
    "    print(\"Running on Colab/Kaggle\")\n",
    "    download_data()\n",
    "    install_dependencies()\n",
    "else:\n",
    "    print(\"Not running in Colab/Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Model Selection\n",
    "\n",
    "Before selecting a model for your task, it’s important to reflect on your goals, resources, and technical environment. Here are key questions to guide your decision:\n",
    "\n",
    "Do you want to run the model on your own hardware?\n",
    "- Open-source models (like LLaMA, Mistral, or Qwen) can often be downloaded and run locally. This gives you more control over data privacy and fine-tuning but also requires sufficient computing power (especially GPU memory) and setup effort.\n",
    "\n",
    "Do you have funds to use a proprietary model?\n",
    "- Commercial models like OpenAI’s GPT-4, Anthropic’s Claude, or Google’s Gemini offer strong performance and easy-to-use APIs, but they come with usage costs. This can be worthwhile if you want strong performance without managing infrastructure.\n",
    "\n",
    "Do you have strong programming skills?\n",
    "- Open-source models typically require setting up environments (e.g., using PyTorch, HuggingFace Transformers), handling tokenization, managing memory usage, and sometimes fine-tuning. If you’re comfortable with Python and command-line tools, these are manageable. If not, hosted APIs might be easier to work with.\n",
    "\n",
    "**Resources**\n",
    "\n",
    "The following resources give an overview of available models and how they compare on different metrics of performance and cost.\n",
    "\n",
    "- The popular benchmark **Chatbot Arena** _(Chiang et al., 2024)_ provides a live updated elo score based on crowd-sourced pairwise comparisons of their responses.\n",
    "    - Trade-off Plot: https://lmarena.ai/price\n",
    "- The **Stanford HELM Benchmark** evaluates language models on over 50 tasks, including understanding and reasoning. It provides insights into model performance in terms of accuracy, fairness, and robustness.\n",
    "    - https://crfm.stanford.edu/helm/\n",
    "- The **Artificial Analysis** platform provides independent analysis of AI models and API providers. The main metrics are 'intelligence', speed, and price. \n",
    "    - https://artificialanalysis.ai/\n",
    "- The **Open LLM Leaderboard** on Hugging Face runs multiple well-established benchmarks for open-source models hosted on the site. It is most useful to compare smaller open-source LLMs that are not featured on Chatbot Arena or compare different fine-tunes of models. \n",
    "    - https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task**\n",
    "\n",
    "Choose the right model based on your use case. Think about what constraints you have and use the resources provided above. \n",
    "\n",
    "Feel free to make notes in this markdown cell.\n",
    "\n",
    "- TODO note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we are picking**\n",
    "\n",
    "For the fine-tuning pipeline later we have the use case of predicting vote choices in the US presidential election based on ANES survey answer. We want to fine-tune a model for _free_ and therefore do not have a lot of computing resources (Google Colab free tier gives access to one T4 GPU). We therefore use the smallest model in the Llama 3 family, **Llama 3.2 1B**, as our model of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Excel to JSON\n",
    "\n",
    "Typical data that is processed by LLMs is assumed to be in JSON format. JSON is a standardized, lightweight, and language-independent format that allows structured data—like text, numbers, lists, or nested objects—to be easily transmitted between systems. Most APIs for LLMs expect JSON because it's both human-readable and machine-readable, making it ideal for sending configuration settings, user input, or model instructions in a clear and organized way.\n",
    "\n",
    "To get a feeling for how data is structured in JSON as opposed to tabular data (Excel, CSV) we want to convert a small table to JSON in this task.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Excel participants table:\n",
    "|   Age | Gender | Education | Occupation |\n",
    "|------:|:-------|:----------|:-----------|\n",
    "|    30 | male   | bachelor  | accountant |\n",
    "|    24 | female | master    | engineer   |\n",
    "\n",
    "Converted to JSON:\n",
    "```json\n",
    "{\n",
    "    \"participants\": [\n",
    "        {\n",
    "            \"age\": 30,\n",
    "            \"gender\": \"male\",\n",
    "            \"education\": \"bachelor\",\n",
    "            \"occupation\": \"accountant\"\n",
    "        },\n",
    "        {\n",
    "            \"age\": 24,\n",
    "            \"gender\": \"female\",\n",
    "            \"education\": \"master\",\n",
    "            \"occupation\": \"engineer\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task**\n",
    "\n",
    "Given the following table of participant answers in a survey:\n",
    "\n",
    "|   Age | Gender   | State     | Vote Choice   |\n",
    "|------:|:---------|:----------|:--------------|\n",
    "|    27 | female   | Louisiana | Trump         |\n",
    "|    45 | male     | NaN       | Clinton       |\n",
    "|   NaN | female   | Ohio      | Non-voter     |\n",
    "\n",
    "What would a corresponding JSON entry look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type up your answer here\n",
    "\n",
    "json_data = {\n",
    "    \"participants\": [\n",
    "        # TODO\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is done in practice**\n",
    "\n",
    "Of course, tables are not manually converted into JSON data in practice Typically python libraries are used to handle data manipulation tasks. In this short course, we will use the popular library _pandas_ for parsing tabular data. In the following, two ways of transforming Excel data into JSON data are shown using the library.\n",
    "\n",
    "Records orientation\n",
    "- Each row of the table is converted into a JSON object, with the column names as keys and the row values as the corresponding values. This results in a list of JSON objects, where each object represents a row in the table.\n",
    "\n",
    "Columns orientation\n",
    "- The table is converted into a JSON object where each key is a column name, and the value is an object containing the index-value pairs for that column. This orientation is useful when you want to access data by columns rather than by rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the popular data manipulation library pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file into a DataFrame, using the first column as the index\n",
    "data = pd.read_excel(\"data/json_task.xlsx\", index_col=0)\n",
    "\n",
    "# Convert the DataFrame to JSON using records orientation\n",
    "# Each row is converted into a JSON object with column names as keys\n",
    "print(\"--- Records Orientation ---\")\n",
    "print(data.to_json(orient=\"records\"))\n",
    "print()\n",
    "\n",
    "# Convert the DataFrame to JSON using columns orientation\n",
    "# Each column is converted into a JSON object with index-value pairs\n",
    "print(\"--- Columns Orientation ---\")\n",
    "print(data.to_json(orient=\"columns\"))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Prompt Design\n",
    "\n",
    "In this exercise, we want to convert our JSON survey participant data, into the structure we will give to the LLM. We will use an _instruction-tuned_ LLM, as they are specifically trained to follow human instructions, as opposed to base models, which predict the next token in a continuous text. Instruction-tuned models use a _chat-format_ as input. These _instruction-tuned_ LLMs often follow a similar prompt format, described in the following.\n",
    "\n",
    "**The OpenAI Chat Format**\n",
    "\n",
    "The OpenAI JSON chat format has become the standard way to structure conversations with AI models. This format is used not only by OpenAI's models but also by many open-source models available on Hugging Face. This standardized format makes it easy to switch between different models (OpenAI, open-source models on Hugging Face, etc.), maintain conversation history in a structured way, and control the behaviour of the model through system messages.\n",
    "\n",
    "The format consists of a list of messages, each with a \"role\" and \"content\":\n",
    "- **role**: Identifies who is speaking (system, user, or assistant)\n",
    "- **content**: Contains the actual message text\n",
    "\n",
    "Overview of the three roles:\n",
    "- The **system** role provides initial instructions to the model. It sets the behaviour and context for the AI assistant. It defines the assistant's role, capabilities, and constraints. The system message is not shown to the end user, when using chatbots like ChatGPT, but guides how the model responds.\n",
    "- The **user** role represents inputs from the human.\n",
    "- The **assistant** role contains the model's responses.\n",
    "\n",
    "**Example Conversion**\n",
    "\n",
    "An example of how to construct a prompt based on survey data in JSON. Here we only consider one participant to make the example more manageable.\n",
    "\n",
    "Survey data:\n",
    "```json\n",
    "{\n",
    "    \"participants\": [\n",
    "        {\n",
    "            \"age\": 30,\n",
    "            \"gender\": \"male\",\n",
    "            \"education\": \"bachelor\",\n",
    "            \"occupation\": \"accountant\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "An example prompting setup where we want the LLM to be a survey participant that answers questions.\n",
    "```json\n",
    "{\n",
    "    \"prompt\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a survey participant. Reply to the user's question with a short answer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your age?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am 30 years old.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your gender?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am a male.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your occupation?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am an accountant.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task**\n",
    "\n",
    "Using the same survey data as provided in the example above. Create a prompting setup that tasks the LLM with predicting the vote choice of the participant instead.\n",
    "\n",
    "Survey data:\n",
    "```json\n",
    "{\n",
    "    \"participants\": [\n",
    "        {\n",
    "            \"age\": 30,\n",
    "            \"gender\": \"male\",\n",
    "            \"education\": \"bachelor\",\n",
    "            \"occupation\": \"accountant\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Questions to consider:\n",
    "- What should the assistant answer look like?\n",
    "- Should the model act as an expert in a certain field? Who does it substitute?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type up your answer here\n",
    "\n",
    "json_prompt = {\n",
    "    \"prompt\": [\n",
    "        # TODO\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using our prompt with an LLM**\n",
    "\n",
    "We can test our prompt with a small open-source model. To do this do _not_ include the \"assistant\" answer JSON object you want the LLM to answer in the prompt. The assistant answer will instead be generated by the LLM, which we chose back in Exercise 1 and we use the instruction-tuned version of the model, as described earlier in this exercise. \n",
    "\n",
    "We load the model _Llama-3.2-1B-Instruct_ with the `transformers` library. The transformers library is a popular open-source library developed by Hugging Face that provides pre-trained models for natural language processing (NLP) and other machine learning tasks. It includes tools for fine-tuning, inference optimization, and model deployment. In this notebook, we'll use transformers to load and run the Llama-3.2-1B-Instruct model for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pipeline module from the transformers library\n",
    "# This provides a simple way to use pre-trained models for various NLP tasks\n",
    "# such as text generation, without having to manually handle model loading and inference\n",
    "from transformers import pipeline\n",
    "\n",
    "# id of a model hosted on Hugging Face\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"  # NOTE: we use the unsloth version of the model, because it does not require an API key, as opposed to the official meta-llama version\n",
    "\n",
    "# Create a pipeline for an instruct model using the json_prompt as input\n",
    "instruct_pipeline = pipeline(\n",
    "    \"text-generation\",  # Specifies the task type for the pipeline (generating text)\n",
    "    model=model_id,     # Sets which model to use (Llama-3.2-1B-Instruct in this case)\n",
    "    device_map=\"auto\",  # Automatically determines the best device (CPU/GPU) for running the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the model, we now generate 10 responses to our prompt `json_prompt`. The responses can differ, as they are sampled probabilistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response using the instruct model pipeline\n",
    "responses = instruct_pipeline(json_prompt[\"prompt\"], num_return_sequences=10)\n",
    "\n",
    "# Print the generated response\n",
    "print(\"10 different generated responses:\")\n",
    "[resp[\"generated_text\"][-1] for resp in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Pipeline\n",
    "\n",
    "In the following we go through the steps required to setup a fine-tuning pipeline to impute missing survey data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "\n",
    "The following is the technical setup required for the fine-tuning pipeline, when running the notebook on Google Colab or Kaggle. \n",
    "\n",
    "This setup is build to be runnable even when the exercise part of this notebook has not been run. Therefore some code is repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional colab/kaggle setup\n",
    "\n",
    "# Importing the sys module to access system-specific parameters and functions\n",
    "import sys\n",
    "# Importing the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Downloads the data from the GitHub repository of this notebook\n",
    "def download_data():\n",
    "    !git clone https://github.com/tobihol/aapor-finetuning.git\n",
    "    %mv aapor-finetuning/data/ .\n",
    "    %rm -rf aapor-finetuning/\n",
    "    return\n",
    "\n",
    "# This function checks if CUDA (GPU support) is available and installs necessary dependencies for fine-tuning\n",
    "# If CUDA is not available, it warns the user to select a GPU runtime\n",
    "# The function installs the following packages:\n",
    "# - bitsandbytes: Library for quantization and memory-efficient operations\n",
    "# - accelerate: Library for distributed training and mixed precision\n",
    "# - transformers: Hugging Face's library for working with pre-trained models\n",
    "# - datasets: Library for accessing and processing datasets\n",
    "# - evaluate: Library for model evaluation\n",
    "# - peft: Parameter-Efficient Fine-Tuning library\n",
    "# - trl: Transformer Reinforcement Learning library\n",
    "# - scikit-learn: Machine learning library for evaluation metrics\n",
    "# - wandb: Weights & Biases for experiment tracking\n",
    "def install_dependencies():\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "      print(\"CUDA is not available. \\nPick a GPU before running this notebook. \\nGo to 'Runtime' -> 'Change runtime type' to do this. (Colab)\")\n",
    "      return \n",
    "    %pip install bitsandbytes\n",
    "    %pip install accelerate\n",
    "    %pip install transformers\n",
    "    %pip install datasets\n",
    "    %pip install evaluate\n",
    "    %pip install peft\n",
    "    %pip install trl\n",
    "    %pip install evaluate\n",
    "    %pip install scikit-learn\n",
    "    %pip install wandb\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def is_running_in_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "def is_running_in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "if is_running_in_colab() or is_running_in_kaggle():\n",
    "    print(\"Running on Colab/Kaggle\")\n",
    "    install_dependencies()\n",
    "else:\n",
    "    print(\"Not running in Colab/Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import the installed dependencies. Before we import any libraries we will set a seed.\n",
    "Setting a consistent seed ensures that:\n",
    "- Model initialization will be the same across runs\n",
    "- Data shuffling will follow the same order\n",
    "- Random operations like dropout will behave consistently\n",
    "- This helps in debugging and comparing model performance across experiments\n",
    "\n",
    "The transformers.set_seed() function sets a random seed for reproducibility across multiple libraries:\n",
    "- PyTorch (both CPU and GPU operations)\n",
    "- NumPy (for numerical operations)\n",
    "- Python's random module\n",
    "- TensorFlow (if used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "seed = 24 # Please set your own favorite seed!\n",
    "\n",
    "# set the seed\n",
    "transformers.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation\n",
    "We use 2016 American National Election Studies survey data. Specifically, the subset of data Argyle et al. (2022) used in study 2 (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JPV20K).\n",
    "\n",
    "The code below loads the 2016 American National Election Studies survey data from a CSV file,\n",
    "displays it as a DataFrame, and then performs exploratory analysis on it. This dataset contains\n",
    "demographic information, political views, and voting behavior of respondents from the 2016 US\n",
    "presidential election, which will be used to build a predictive model for voting choices\n",
    "(Trump, Clinton, or Non-voter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for data manipulation and analysis\n",
    "# This library provides data structures like DataFrames that make working with structured data easy\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df_survey = pd.read_csv(\"data/2016_anes_argyle.csv\")\n",
    "\n",
    "# Display some rows of the DataFrame\n",
    "df_survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Exploration**\n",
    "\n",
    "In the cell below, we're examining the structure and content of our survey dataset:\n",
    "\n",
    "1. We use `df_survey.info()` to get a summary of the DataFrame, which will show:\n",
    "   - Total number of entries (rows)\n",
    "   - Column names and their data types\n",
    "   - Number of non-null values in each column\n",
    "   - Memory usage of the DataFrame\n",
    "\n",
    "This information helps us understand what kind of data we're working with and identify any potential issues like missing values that need to be addressed during preprocessing. It's an essential step before building our model to predict voting behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection and Target Variable**\n",
    "\n",
    "In the next cell, we're defining which columns from our dataset will be used:\n",
    "\n",
    "1. We create a list called `features` containing the demographic and political variables we'll use as predictors:\n",
    "   - Demographic information: race, age, gender, state\n",
    "   - Political attributes: discussion habits, ideology, party affiliation\n",
    "   - Religious behavior: church attendance\n",
    "   - Attitudes: political interest, patriotism\n",
    "\n",
    "2. We define `label` as \"ground_truth\", which represents the actual voting choice of respondents (Trump, Clinton, or Non-voter)\n",
    "\n",
    "This selection of features will be used to train our model to predict voting behavior based on these characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"race\",\n",
    "    \"discuss_politics\",\n",
    "    \"ideology\",\n",
    "    \"party\",\n",
    "    \"church_goer\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"political_interest\",\n",
    "    \"patriotism\",\n",
    "    \"state\",\n",
    "]\n",
    "label = \"ground_truth\" # this is the vote choice in this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "In the next cell, we prepare our data for modeling by:\n",
    "\n",
    "1. Converting the 'age' column to string type to treat it as a categorical variable\n",
    "2. Filling all missing values (NaN) with the string \"missing\" so they can be treated as their own category\n",
    "3. This preprocessing step ensures our data is ready for the machine learning model and handles the missing values appropriately\n",
    "\n",
    "This approach allows us to include all data points in our analysis without dropping rows that have missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey_processed = (\n",
    "    df_survey\n",
    "    .astype({\"age\": str})\n",
    "    .fillna(\"missing\")\n",
    ")\n",
    "df_survey_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "Any manipulation of the training data should be done in the step.\n",
    "\n",
    "We split our processed survey data into training and testing sets:\n",
    "\n",
    "1. We use scikit-learn's train_test_split function to divide our data, with 80% for training and 20% for testing\n",
    "2. We set a random seed for reproducibility\n",
    "3. There's commented code showing how we could modify the training data for different experiments (e.g., excluding Republican voters)\n",
    "4. We convert our pandas DataFrames to Hugging Face Dataset objects for compatibility with transformer models\n",
    "5. The datasets are organized in a DatasetDict with \"train\" and \"test\" keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "df_train, df_test = train_test_split(\n",
    "    df_survey_processed,  # The processed survey dataframe to split into train and test sets\n",
    "    test_size=0.2,        # Allocate 20% of the data to the test set\n",
    "    random_state=seed,    # Set a random seed for reproducibility of the split\n",
    ")\n",
    "\n",
    "# we can modify the training data here to do different experiments\n",
    "# for example excluding republican voters\n",
    "# leans_republican = df_train[\"party\"].apply(lambda x: \"Republican\" in x)\n",
    "# df_train = df_train[~leans_republican]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(df_test, preserve_index=False),\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Design\n",
    "\n",
    "We will use an instruction-tuned model and will therefore define an instruction prompt here, like we did in exercise 3.\n",
    "\n",
    "We design a prompt for our instruction-tuned model to classify voter behavior.\n",
    "The prompt will include:\n",
    "1. A clear instruction for the classification task\n",
    "2. Context about the data (2016 survey answers from American National Election Studies)\n",
    "3. Expected output format (one of three labels: 'Trump', 'Clinton', or 'Non-voter')\n",
    "4. A structured format to present the survey data to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"Please perform a classification task. \"\n",
    "    \"Given the 2016 survey answers from the American National Election Studies, \"\n",
    "    \"return which candidate the person voted for. \"\n",
    "    \"Return a label from ['Trump', 'Clinton', 'Non-voter'] only without any other text.\\n\"\n",
    ")\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a mapping between our internal column names and more human-readable labels\n",
    "to make the prompts more understandable for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_map = {\n",
    "    \"race\": \"Race\",\n",
    "    \"discuss_politics\": \"Discusses politics\",\n",
    "    \"ideology\": \"Ideology\",\n",
    "    \"party\": \"Party\",\n",
    "    \"church_goer\": \"Church\",\n",
    "    \"age\": \"Age\",\n",
    "    \"gender\": \"Gender\",\n",
    "    \"political_interest\": \"Political interest\",\n",
    "    \"patriotism\": \"American Flag\",\n",
    "    \"state\": \"State\",\n",
    "    \"ground_truth\": \"Vote\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to build prompt-completion pairs for our model.\n",
    "This function will:\n",
    "1. Take a row of survey data and format it using our column name mapping\n",
    "2. Create a structured prompt with system and user messages\n",
    "3. Set the ground truth vote as the expected completion\n",
    "4. Return the formatted prompt-completion pair for model training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_completion(\n",
    "    row: dict,\n",
    "    system_prompt: str = instruction,\n",
    ") -> list[list[dict]]:\n",
    "    user_prompt = \"\\n\".join(\n",
    "        [f\"{column_name_map[k]}: {v}\" for k, v in row.items() if k != label]\n",
    "    )\n",
    "    assistant_prompt = row[label]\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            },\n",
    "        ],\n",
    "        \"completion\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "build_prompt_completion(\n",
    "    row=dataset[\"train\"][0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert our dataset to the format needed for LLM training\n",
    "The code below:\n",
    "1. Applies our build_prompt_completion function to each row in the dataset\n",
    "2. Removes the original feature columns and label column\n",
    "3. Creates a new dataset with only the prompt-completion pairs\n",
    "\n",
    "This transforms our tabular data into a format suitable for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_llm = dataset.map(build_prompt_completion).remove_columns(features+[label])\n",
    "dataset_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "We use `Llama-3.2-1B-Instruct` for this experiment, because:\n",
    "1. It's a small model (1B parameters) that can run on limited hardware (one T4 GPU in our case, see exercise 1)\n",
    "2. It's an instruction-tuned model, which makes it suitable for our classification task (see exercise 3)\n",
    "3. It provides a good balance between performance and computational requirements\n",
    "4. It's based on the Llama architecture which has shown strong performance on various NLP tasks\n",
    "5. The model is open and accessible, making it suitable for educational purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tokenizer\n",
    "\n",
    "The code below loads the tokenizer for our model.\n",
    "The tokenizer is responsible for converting text into tokens that the model can understand.\n",
    "We use `AutoTokenizer` to automatically select the appropriate tokenizer for our chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,  # The ID of the model to load the tokenizer for\n",
    "    # revision=revision, # NOTE: revision should be set for an reproducible experiment \n",
    "    trust_remote_code=True,  # Allow the tokenizer to execute remote code from the model repository\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Quantized Model\n",
    "\n",
    "Quantization reduces the memory required to store the model (Dettmers et al., 2022). Typically a model is stored in 16-bit precision, therefore for a 1B parameter model:\n",
    "\n",
    "$$\\frac{16 \\text{ bits}}{8 \\text{ bits/byte}} \\times 1 \\times 10^9 \\text{ parameters} = 2 \\text{ GB of VRAM}$$\n",
    "\n",
    "With 4-bit quantization, all parameters are stored in 4-bit precision, reducing the memory requirement to:\n",
    "\n",
    "$$\\frac{4 \\text{ bits}}{8 \\text{ bits/byte}} \\times 1 \\times 10^9 \\text{ parameters} = 0.5 \\text{ GB of VRAM}$$\n",
    "\n",
    "The code below loads our model with 4-bit quantization using the BitsAndBytesConfig,\n",
    "which significantly reduces memory usage while maintaining reasonable performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# load model in 4bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,  # The ID of the model to load\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,  # Enable 4-bit quantization to reduce memory usage\n",
    "    ),\n",
    "    trust_remote_code=True,  # Allow execution of remote code from the model repository\n",
    "    device_map=\"auto\",  # Automatically determine whether to use CPU or GPU\n",
    ")\n",
    "\n",
    "# Overview of the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA\n",
    "Low-Rank Adapters (LoRA) are a parameter efficient fine-tuning method (Hu et al., 2021). Instead of finetuning all model weights, LoRA finetunes the weights of the adapter layers only. This requires less memory and allows for faster finetuning.\n",
    "\n",
    "![LoRA Diagram](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png) \n",
    "\n",
    "The code below sets up the LoRA configuration for fine-tuning the model.\n",
    "It defines parameters like rank (r) which controls the size of low-rank matrices,\n",
    "alpha which scales the LoRA contribution, dropout for regularization,\n",
    "and specifies that we're applying LoRA to all linear layers in the model.\n",
    "This configuration will be used to create trainable adapter layers while\n",
    "keeping most of the original model weights frozen during fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# key hyperparameters\n",
    "lora_rank = 8\n",
    "lora_alpha = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,  # Rank of the low-rank matrices\n",
    "    lora_alpha=lora_alpha,  # Scaling factor for the LoRA contribution\n",
    "    lora_dropout=0.05,  # Dropout probability for regularization (default)\n",
    "    bias=\"none\",  # Don't apply LoRA to bias parameters (default)\n",
    "    task_type=TaskType.CAUSAL_LM,  # Specify that we're fine-tuning a causal language model\n",
    "    target_modules=\"all-linear\",  # Apply LoRA to all linear layers in the model\n",
    ")\n",
    "\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "For simplicity we evaluate on the _first token_ of the LLM generated answer only. If the first token with the highest probability matches the true first token we have classified correctly. We therefore can use typical classification metrics, i.e., _Accuracy_ and _F1 score_.\n",
    "\n",
    "### Accuracy Score\n",
    "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $TP$: True positive\n",
    "- $TN$: True negative\n",
    "- $FP$: False positive\n",
    "- $FN$: False negative\n",
    "\n",
    "\n",
    "### F1 Score\n",
    "F1 score is the harmonic mean of precision and recall, providing a balance between these two metrics:\n",
    "$$\n",
    "\\text{F1 score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\text{Precision} = TP / (TP + FP)$ - the ability of the model to avoid false positives\n",
    "- $\\text{Recall} = TP / (TP + FN)$ - the ability of the model to find all positive samples\n",
    "\n",
    "Macro F1 score calculates the F1 score independently for each class and then takes the average, treating all classes equally regardless of their support:\n",
    "$$\n",
    "\\text{Macro F1} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{F1}_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads evaluation metrics from the Hugging Face 'evaluate' library.\n",
    "\n",
    "These metrics will be used to evaluate our fine-tuned model's performance on the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "hf_metrics = [\n",
    "    evaluate.load(\"accuracy\"),\n",
    "    evaluate.load(\"f1\", average=\"macro\"),\n",
    "]\n",
    "\n",
    "# give description of each metric used\n",
    "for metric in hf_metrics:\n",
    "    print(metric.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Training Setup\n",
    "This is python code is not covered in the workshop and such sets up some additional details for fine-tuning and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# set pad token id\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "if getattr(model.config, \"pad_token_id\") is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "labels = df_survey_processed.ground_truth.unique()\n",
    "\n",
    "first_token_ids = [\n",
    "    tokenizer.encode(label, add_special_tokens=False)[0]\n",
    "    for label in labels\n",
    "]\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    # Return highest probability token in answer options\n",
    "    # return logits[:, :, first_token_ids]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "y_probas = []\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    labels = labels[:, 1:]\n",
    "    preds = preds[:, :-1]\n",
    "\n",
    "    # -100 is a default value for ignore_index used by DataCollatorForCompletionOnlyLM\n",
    "    mask = labels == -100\n",
    "    labels[mask] = tokenizer.pad_token_id\n",
    "    preds[mask] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    results = {}\n",
    "    for metric in hf_metrics:\n",
    "        results |= metric.compute(predictions=preds[~mask], references=labels[~mask])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "The code below sets up and runs a fine-tuning process for a language model:\n",
    "1. Configures SFTTrainer with hyperparameters (learning rate, batch size, epochs)\n",
    "2. Sets up Weights & Biases (wandb) for experiment tracking\n",
    "3. Creates training arguments with evaluation strategy and logging settings\n",
    "4. Initializes the SFTTrainer with the model, datasets, and LoRA configuration\n",
    "5. Starts the training process to fine-tune the model on the ANES 2016 survey data\n",
    "\n",
    "To run the training without wandb logging, set `wandb.init(mode='disabled')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from datetime import datetime\n",
    "\n",
    "# key hyperparameters\n",
    "learning_rate = 2e-5  # Learning rate for optimizer - controls how quickly model parameters are updated\n",
    "batch_size = 8        # Number of samples processed in each training batch\n",
    "epochs = 1            # Number of complete passes through the training dataset (NOTE you should run multiple epochs in practice)\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "dataset_name = \"argyle_anes_2016\"\n",
    "run_name = f\"{model_id}_{dataset_name}_seed_{seed}_{now}\"\n",
    "\n",
    "# wandb.init(\n",
    "#     mode='disabled',\n",
    "# )\n",
    "wandb.init(\n",
    "    project=\"aapor-finetuning\",\n",
    "    name=run_name,\n",
    ")\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # training parameters\n",
    "    per_device_train_batch_size=batch_size,  # number of samples per batch on each device during training\n",
    "    per_device_eval_batch_size=batch_size,   # number of samples per batch on each device during evaluation\n",
    "    num_train_epochs=epochs,                 # total number of training epochs\n",
    "    \n",
    "    # evaluation settings\n",
    "    do_eval=True,                            # whether to run evaluation\n",
    "    eval_strategy=\"steps\",                   # when to run evaluation (after certain steps)\n",
    "    eval_steps=1 / 3,                        # evaluate after each third of an epoch\n",
    "    \n",
    "    # logging configuration\n",
    "    logging_steps=10,                        # log metrics every 10 steps\n",
    "    report_to=\"wandb\",                       # send logs to Weights & Biases\n",
    "    run_name=run_name,                       # name of the run for tracking\n",
    "    \n",
    "    output_dir=\"./results\",                  # directory to save model checkpoints and logs\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                                # The pre-trained model to fine-tune\n",
    "    train_dataset=dataset_llm[\"train\"],         # Training dataset with prompts and completions\n",
    "    eval_dataset=dataset_llm[\"test\"],           # Evaluation dataset for testing model performance\n",
    "    args=training_args,                         # Training configuration settings\n",
    "    peft_config=lora_config,                    # LoRA configuration for parameter-efficient fine-tuning\n",
    "    compute_metrics=compute_metrics,            # Function to compute evaluation metrics (not covered in this course)\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,  # Function to preprocess model outputs for metric calculation (not covered in this course)\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fine-tuning results can be found live at: https://wandb.ai/tobihol/aapor-finetuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
