{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional google colab setup\n",
    "import sys\n",
    "\n",
    "\n",
    "def colab_install():\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "      print(\"CUDA is not available. \\nPick a GPU before running this notebook. \\nGo to 'Runtime' -> 'Change runtime type' to do this.\")\n",
    "      return \n",
    "    %pip install transformers\n",
    "    %pip install datasets\n",
    "    %pip install peft\n",
    "    %pip install bitsandbytes\n",
    "    %pip install evaluate\n",
    "    %pip install wandb\n",
    "    %pip install numpy==1.* # ligtheval is not compatible with 2.0 TODO: check this\n",
    "    %pip install lighteval\n",
    "    return\n",
    "\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Install required packages\n",
    "    colab_install()\n",
    "else:\n",
    "    print(\"Not running in Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "transformers.set_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    %cd survai-finetuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "!curl -L -o 2016_anes_argyle.pkl https://github.com/tobihol/survai-finetuning/raw/main/2016_anes_argyle.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey = pd.read_pickle(\"2016_anes_argyle.pkl\")\n",
    "df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics\n",
    "df_survey.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"race\",\n",
    "    \"discuss_politics\",\n",
    "    \"ideology\",\n",
    "    \"party\",\n",
    "    \"church_goer\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"political_interest\",\n",
    "    \"patriotism\",\n",
    "    \"state\",\n",
    "]\n",
    "label = \"ground_truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tread missing values as a category \n",
    "df_survey_processed = (\n",
    "    df_survey\n",
    "    .astype({\"age\": str})\n",
    "    .fillna(\"missing\")\n",
    ")\n",
    "df_survey_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"Please perform a classification task. \"\n",
    "    + \"Given the 2016 survey answers from the American National Election Studies, \"\n",
    "    + \"return which candiate the person voted for. \"\n",
    "    + \"Return a label from ['Trump', 'Clinton', 'Non-voter'] only without any other text.\\n\"\n",
    ")\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_map = {\n",
    "    \"race\": \"Race\",\n",
    "    \"discuss_politics\": \"Discusses politics\",\n",
    "    \"ideology\": \"Ideology\",\n",
    "    \"party\": \"Party\",\n",
    "    \"church_goer\": \"Church\",\n",
    "    \"age\": \"Age\",\n",
    "    \"gender\": \"Gender\",\n",
    "    \"political_interest\": \"Political interest\",\n",
    "    \"patriotism\": \"American Flag\",\n",
    "    \"state\": \"State\",\n",
    "    \"ground_truth\": \"Vote\",\n",
    "}\n",
    "\n",
    "def create_prompt(row):\n",
    "    prompt = instruction\n",
    "    prompt += \"\\n\".join([f\"{column_name_map[k]}: {v}\" for k, v in row.items()])\n",
    "    return prompt\n",
    "\n",
    "text_series = df_survey_processed[features].apply(create_prompt, axis=\"columns\")\n",
    "label_series = df_survey_processed[label]\n",
    "\n",
    "df_prompts = pd.DataFrame({\"text\": text_series, \"label\": label_series})\n",
    "df_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prompts.text.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prompts.label.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_survey_processed, test_size=0.2, random_state=24)\n",
    "dataset_cls = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(df_test, preserve_index=False),\n",
    "})\n",
    "dataset_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_prompts, test_size=0.2, random_state=24)\n",
    "dataset_llm = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(df_test, preserve_index=False),\n",
    "})\n",
    "dataset_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection:\n",
    "- https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n",
    "- https://lmarena.ai/\n",
    "- https://crfm.stanford.edu/helm/\n",
    "    - Imputation Benchmark: https://crfm.stanford.edu/helm/classic/latest/#/groups/entity_data_imputation\n",
    "\n",
    "Reproducibility:\n",
    "- (TODO how model revisions)\n",
    "- LLM training/inference is the wild west: the are a ton of differnt libraries/wrappers where each one can implement different changes your evaluations results. These libaries also get often fixed and updated in major ways which can break your pipeline or change results. (TODO add tokenisation problem that I had during my thesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Versioning Problem Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Answer Extraction Problem\n",
    "\n",
    "<!-- - https://arxiv.org/pdf/2307.09702, https://github.com/dottxt-ai/outlines -->\n",
    "Different modles for answer extraction:\n",
    "- https://blog.eleuther.ai/multiple-choice-normalization/\n",
    "- https://github.com/huggingface/lighteval\n",
    "\n",
    "Problem 1: How many tokens are need to answer the question:\n",
    "- One token solutions:\n",
    "    - less compute intensive\n",
    "    - do not require normalisation\n",
    "    - only works if all first token are destinct\n",
    "- Multi token solutions:\n",
    "    - more compute intensive (multiplied by number of lables)\n",
    "    - might require normalisation\n",
    "    - does not require all first tokens to be distinct\n",
    "    \n",
    "Problem 2: How to evaluate multi token extraction (see code below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lighteval.metrics.metrics_sample import LoglikelihoodAcc\n",
    "from lighteval.metrics.normalizations import (\n",
    "    LogProbCharNorm,\n",
    "    # LogProbTokenNorm,\n",
    "    # LogProbPMINorm,\n",
    ")\n",
    "from lighteval.tasks.requests import Doc\n",
    "import numpy as np\n",
    "\n",
    "acc_metric = LoglikelihoodAcc(\n",
    "    # LogProbCharNorm(ignore_first_space=False),\n",
    ")\n",
    "\n",
    "choices = [\"Trump\", \"Clinton\", \"Non-voter\"]\n",
    "log_prob_predictions = np.log([0.34, 0.33, 0.32])\n",
    "correct_choice = \"Non-voter\"\n",
    "\n",
    "doc = Doc(query=\"...\", choices=choices, gold_index=[choices.index(correct_choice)])\n",
    "\n",
    "\n",
    "acc_without_normalisation = LoglikelihoodAcc(\n",
    "    # LogProbCharNorm(ignore_first_space=False),\n",
    ").compute(\n",
    "    gold_ixs=doc.gold_index,\n",
    "    choices_logprob=log_prob_predictions,\n",
    "    unconditioned_logprob=None,\n",
    "    choices_tokens=None,\n",
    "    formatted_doc=doc,\n",
    ")\n",
    "print(f\"Accuracy score without normalisation: {acc_without_normalisation}\")\n",
    "\n",
    "acc_with_normalisation = LoglikelihoodAcc(\n",
    "    LogProbCharNorm(ignore_first_space=False),\n",
    ").compute(\n",
    "    gold_ixs=doc.gold_index,\n",
    "    choices_logprob=log_prob_predictions,\n",
    "    unconditioned_logprob=None,\n",
    "    choices_tokens=None,\n",
    "    formatted_doc=doc,\n",
    ")\n",
    "print(f\"Accuracy score with normalisation: {acc_with_normalisation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Contintuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"EleutherAI/pythia-70m\"\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "# revision = \"8d308458221c84f2b793d9b820d72e2c10159630\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # revision=revision,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruct_tokenize_function(examples):\n",
    "    prompt = [\n",
    "        {\"role\": \"user\", \"content\": examples[\"text\"]},\n",
    "    ]\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": examples[\"label\"],\n",
    "        }\n",
    "    )\n",
    "    inputs_ids = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    attention_mask = np.ones_like(inputs_ids)\n",
    "    return {\n",
    "        \"input_ids\": inputs_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "def basic_tokenize_function(examples):\n",
    "    prompt = f\"{examples['text']} \\nVote: {examples['label']} {tokenizer.eos_token}\"\n",
    "    return tokenizer(prompt)\n",
    "\n",
    "\n",
    "tokenized_dataset_llm = dataset_llm.map(basic_tokenize_function).remove_columns(\n",
    "    [\"text\", \"label\"]\n",
    ")\n",
    "tokenized_dataset_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model in 4bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "if getattr(model.config, \"pad_token_id\") is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def instruct_tokenization(\n",
    "    data: DatasetDict,\n",
    "    tokenizer: AutoTokenizer,\n",
    ") -> Tuple[DatasetDict, Dataset]:\n",
    "    def tokenize_function(examples, is_inference=False):\n",
    "        prompt = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"text\"]},\n",
    "        ]\n",
    "        if not is_inference:\n",
    "            prompt.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": examples[\"label\"],\n",
    "                }\n",
    "            )\n",
    "        inputs_ids = tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            add_generation_prompt=is_inference,\n",
    "        )\n",
    "        attention_mask = np.ones_like(inputs_ids)\n",
    "        return {\n",
    "            \"input_ids\": inputs_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "    column_names = list(data.column_names.values())[0]\n",
    "    training_data = data.map(tokenize_function, remove_columns=column_names)\n",
    "    from functools import partial\n",
    "\n",
    "    inference_data = data.map(\n",
    "        partial(tokenize_function, is_inference=True), remove_columns=column_names\n",
    "    )\n",
    "\n",
    "    answer_tokens = list(\n",
    "        {\n",
    "            training_ids[len(inference_ids)]\n",
    "            for inference_ids, training_ids in zip(\n",
    "                inference_data[\"train\"][\"input_ids\"]\n",
    "                + inference_data[\"test\"][\"input_ids\"],\n",
    "                training_data[\"train\"][\"input_ids\"]\n",
    "                + training_data[\"test\"][\"input_ids\"],\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    assert len(answer_tokens) == len(\n",
    "        set(data[\"test\"][\"label\"] + data[\"train\"][\"label\"])\n",
    "    )\n",
    "\n",
    "    return training_data, inference_data, answer_tokens\n",
    "\n",
    "\n",
    "training_data, inference_data, answer_tokens = instruct_tokenization(\n",
    "    dataset_llm, tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "\n",
    "# TODO: make other metrics work\n",
    "hf_metrics = [\n",
    "    evaluate.load(\"accuracy\"),\n",
    "    # evaluate.load(\"f1\"),\n",
    "    # evaluate.load(\"precision\"),\n",
    "    # evaluate.load(\"recall\"),\n",
    "    # evaluate.load(\"confusion_matrix\"),\n",
    "]\n",
    "sklearn_metrics = {\n",
    "    # \"accuracy\": metrics.accuracy_score,\n",
    "    # \"balanced_accuracy\": metrics.balanced_accuracy_score,\n",
    "    # \"f1_weighted\": partial(metrics.f1_score, average=\"weighted\"),\n",
    "    # \"confusion_matrix\": metrics.confusion_matrix,\n",
    "}\n",
    "\n",
    "pred_slice_ids = [\n",
    "    (len(inference_ids), len(training_ids) - 1)\n",
    "    for inference_ids, training_ids in zip(\n",
    "        inference_data[\"test\"][\"input_ids\"], training_data[\"test\"][\"input_ids\"]\n",
    "    )\n",
    "]  # NOTE: the -1 accounts for the eos token, which is not present for the generation data\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    logits = logits[:, :, answer_tokens].argmax(dim=-1)\n",
    "\n",
    "    return torch.tensor(\n",
    "        answer_tokens,\n",
    "        device=\"cuda\",\n",
    "    )[logits]\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    preds = np.pad(preds, ((0, 0), (1, 0)), mode=\"constant\", constant_values=-100)\n",
    "    labels = np.pad(labels, ((0, 0), (0, 1)), mode=\"constant\", constant_values=-100)\n",
    "\n",
    "    def get_slice(y):\n",
    "        return [\n",
    "            [token for token, label_token in zip(row, label) if label_token != -100][\n",
    "                start_id:end_id\n",
    "            ]\n",
    "            for (start_id, end_id), row, label in zip(pred_slice_ids, y, labels)\n",
    "        ]\n",
    "\n",
    "    y_true = get_slice(labels)\n",
    "    y_pred = get_slice(preds)\n",
    "    # accuracy based on the first token of the vote\n",
    "    y_true = [row[0] for row in y_true]\n",
    "    y_pred = [row[0] for row in y_pred]\n",
    "\n",
    "    results = {}\n",
    "    for metric in hf_metrics:\n",
    "        results |= metric.compute(predictions=y_pred, references=y_true)\n",
    "    for metric_name, metric_func in sklearn_metrics.items():\n",
    "        results[metric_name] = metric_func(y_true=y_true, y_pred=y_pred)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "run_name = f'{model_id}_{now}'\n",
    "\n",
    "# wandb.init(\n",
    "#     mode='disabled',\n",
    "# )\n",
    "\n",
    "wandb.init(\n",
    "    project=\"survai-finetuning\",\n",
    "    name=run_name,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=training_data[\"train\"],\n",
    "    eval_dataset=training_data[\"test\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "\n",
    "        # train/eval settings\n",
    "        num_train_epochs=3,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "\n",
    "        # logging\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"test-run\",\n",
    "    ),\n",
    "    # tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things we did not do\n",
    "\n",
    "Some parts of the pipeline we did not do, because of time constraints, but should be done in pratice:\n",
    "- Hyperparameter search\n",
    "- Cross validation\n",
    "- Reporting multiple seeds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (survai-finetuning)",
   "language": "python",
   "name": "survai-finetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
