{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup\n",
    "The following is the technical setup required to run the exercises 1, 2, and 3 content, when running the notebook on Google Colab or Kaggle. \n",
    "\n",
    "For the fine-tuning we have additional technical setup later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional colab/kaggle setup\n",
    "\n",
    "# Importing the sys module to access system-specific parameters and functions\n",
    "import sys\n",
    "# Importing the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Downloads the data from the GitHub repository of this notebook\n",
    "def download_data():\n",
    "    !git clone https://github.com/tobihol/aapor-finetuning.git\n",
    "    %mv aapor-finetuning/data/ .\n",
    "    %rm -rf aapor-finetuning/\n",
    "    return\n",
    "\n",
    "# Checks if the code is running in a Kaggle environment\n",
    "def is_running_in_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "# Checks if the code is running in a Google Colab environment\n",
    "def is_running_in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "# Check if running in Colab or Kaggle and download data accordingly\n",
    "if is_running_in_colab() or is_running_in_kaggle():\n",
    "    print(\"Running on Colab/Kaggle\")\n",
    "    download_data()\n",
    "else:\n",
    "    print(\"Not running in Colab/Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Model Selection\n",
    "\n",
    "Choose the right model based on your use case.\n",
    "\n",
    "Before selecting a model for your task, it’s important to reflect on your goals, resources, and technical environment. Here are key questions to guide your decision:\n",
    "\n",
    "Do you want to run the model on your own hardware?\n",
    "- Open-source models (like LLaMA, Mistral, or Qwen) can often be downloaded and run locally. This gives you more control over data privacy and fine-tuning, but also requires sufficient computing power (especially GPU memory) and setup effort.\n",
    "\n",
    "Do you have funds to use a proprietary model?\n",
    "- Commercial models like OpenAI’s GPT-4, Anthropic’s Claude, or Google’s Gemini offer strong performance and easy-to-use APIs, but they come with usage costs. This can be worthwhile if you want strong performance without managing infrastructure.\n",
    "\n",
    "Do you have strong programming skills?\n",
    "- Open-source models typically require setting up environments (e.g., using PyTorch, HuggingFace Transformers), handling tokenization, managing memory usage, and sometimes fine-tuning. If you’re comfortable with Python and command-line tools, these are manageable. If not, hosted APIs might be easier to work with.\n",
    "\n",
    "Think about your use case and pick a model. \n",
    "\n",
    "**Resources**\n",
    "\n",
    "The following resources give an overview of available models and how they compare on different metrics of performance and cost.\n",
    "\n",
    "- The popular benchmark **Chatbot Arena** _(Chiang et al., 2024)_  provides a live updated elo score based on crowd-sourced pairwise comparisons of their responses.\n",
    "    - Trade-off Plot: https://lmarena.ai/price\n",
    "- The **Stanford HELM Benchmark** evaluates language models on over 50 tasks, including understanding and reasoning. It provides insights into model performance in terms of accuracy, fairness, and robustness.\n",
    "    - https://crfm.stanford.edu/helm/\n",
    "- The **Artificial Analysis** platform provides independent analysis of AI models and API providers. The main metrics are 'intelligence', speed, and price. \n",
    "    - https://artificialanalysis.ai/\n",
    "- The **Open LLM Leaderboard** on Hugging Face runs multiple well established benchmarks on for open-source models hosted on the site. It is most useful to compare smaller open-source LLMs that are not featured on Chatbot Arena or compare different fine-tunes of models. \n",
    "    - https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "Feel free to make notes here, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we are picking**\n",
    "\n",
    "Our use case is to predict vote choices in the US presidential election based on ANES survey answer. We want to fine-tune a model for _free_ and therefore do not have a lot of computing resources (Google Colab free tier gives access to one T4 GPU). We therefor use the smallest model in the Llama 3 family, **Llama 3.2 1B**, as our model of choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Excel to JSON\n",
    "\n",
    "What would the corresponding JSON entry look like?\n",
    "\n",
    "|   Age | Gender   | State     | Vote Choice   |\n",
    "|------:|:---------|:----------|:--------------|\n",
    "|    27 | female   | Louisiana | Trump         |\n",
    "|    45 | male     | nan       | Clinton       |\n",
    "|   nan | female   | Ohio      | Non-voter     |\n",
    "\n",
    "\n",
    "**Example**\n",
    "\n",
    "Excel\n",
    "|   Age | Gender | Education | Occupation |\n",
    "|------:|:-------|:----------|:-----------|\n",
    "|    30 | male   | bachelor  | accountant |\n",
    "|    24 | female | master    | engineer   |\n",
    "\n",
    "JSON\n",
    "```json\n",
    "{\n",
    "    \"participants\": [\n",
    "        {\n",
    "            \"age\": 30,\n",
    "            \"gender\": \"male\",\n",
    "            \"education\": \"bachelor\",\n",
    "            \"occupation\": \"accountant\"\n",
    "        },\n",
    "        {\n",
    "            \"age\": 24,\n",
    "            \"gender\": \"female\",\n",
    "            \"education\": \"master\",\n",
    "            \"occupation\": \"engineer\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Extra Question:**\n",
    "Are there different ways to construct the JSON?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type up your answer here\n",
    "\n",
    "json_data = {\n",
    "    \"participants\": [\n",
    "        # TODO\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice this would be done automatically,\n",
    "# here is are two different examples of how to do so\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"data/json_task.xlsx\", index_col=0)\n",
    "\n",
    "print(\"--- Records Orientation ---\")\n",
    "print(data.to_json(orient=\"records\"))\n",
    "print()\n",
    "\n",
    "print(\"--- Columns Orientation ---\")\n",
    "print(data.to_json(orient=\"columns\"))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: JSON Prompt\n",
    "\n",
    "Build a prompt for your task.\n",
    "\n",
    "**Questions to consider:**\n",
    "- What should the output look like?\n",
    "- Should the model act as an expert in a certain field? Who does it substitute?\n",
    "\n",
    "\n",
    "**Example use-case:**\n",
    "- Predicting vote choice in the US presidential election based on ANES survey answer\n",
    "\n",
    "**Example Conversion**\n",
    "\n",
    "SURVEY DATA\n",
    "```json\n",
    "{\n",
    "    \"participants\": [\n",
    "        {\n",
    "            \"age\": 30,\n",
    "            \"gender\": \"male\",\n",
    "            \"education\": \"bachelor\",\n",
    "            \"occupation\": \"accountant\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "PROMPT\n",
    "```json\n",
    "{\n",
    "    \"prompt\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a survey participant. Reply to the user's question with a short answer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your age?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am 30 years old.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your gender?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am a male.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your occupation?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am an accountant.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type up your answer here\n",
    "\n",
    "json_prompt = {\n",
    "    \"prompt\": [\n",
    "        # TODO\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our prompt with a small open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# id of a model hosted on Hugging Face\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Create a pipeline for an instruct model using the json_prompt as input\n",
    "instruct_pipeline = pipeline(\"text-generation\", model=model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response using the instruct model pipeline\n",
    "responses = instruct_pipeline(json_prompt[\"prompt\"], num_return_sequences=10)\n",
    "\n",
    "# Print the generated response\n",
    "print(\"10 different generated responses:\")\n",
    "[resp[\"generated_text\"][-1] for resp in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Pipeline\n",
    "In the following we go through the steps required to setup a fine-tuning pipeline to impute missing survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional colab/kaggle setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_dependencies():\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "      print(\"CUDA is not available. \\nPick a GPU before running this notebook. \\nGo to 'Runtime' -> 'Change runtime type' to do this. (Colab)\")\n",
    "      return \n",
    "    %pip install bitsandbytes\n",
    "    %pip install accelerate\n",
    "    %pip install transformers\n",
    "    %pip install datasets\n",
    "    %pip install evaluate\n",
    "    %pip install peft\n",
    "    %pip install trl\n",
    "    %pip install evaluate\n",
    "    %pip install scikit-learn\n",
    "    %pip install wandb\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def is_running_in_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "def is_running_in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "if is_running_in_colab() or is_running_in_kaggle():\n",
    "    print(\"Running on Colab/Kaggle\")\n",
    "    install_dependencies()\n",
    "else:\n",
    "    print(\"Not running in Colab/Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "seed = 24 # Please set your own favorite seed!\n",
    "transformers.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation\n",
    "We use 2016 American National Election Studies survey data. Specifically, the subset of data Argyle et al. (2022) used in study 2 (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JPV20K)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey = pd.read_csv(\"data/2016_anes_argyle.csv\")\n",
    "df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null values and data types\n",
    "df_survey.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"race\",\n",
    "    \"discuss_politics\",\n",
    "    \"ideology\",\n",
    "    \"party\",\n",
    "    \"church_goer\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"political_interest\",\n",
    "    \"patriotism\",\n",
    "    \"state\",\n",
    "]\n",
    "label = \"ground_truth\" # this is the vote choice in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tread missing values as a category \n",
    "df_survey_processed = (\n",
    "    df_survey\n",
    "    .astype({\"age\": str})\n",
    "    .fillna(\"missing\")\n",
    ")\n",
    "df_survey_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split\n",
    "\n",
    "Any manipulation of the training data should be done in the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_survey_processed, test_size=0.2, random_state=seed)\n",
    "\n",
    "# we can modify the training data here to do different experiments\n",
    "# for example excluding republican voters\n",
    "# leans_republican = df_train[\"party\"].apply(lambda x: \"Republican\" in x)\n",
    "# df_train = df_train[~leans_republican]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(df_test, preserve_index=False),\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Design\n",
    "\n",
    "We will use an instruction-tuned model and will therefore define an instruction prompt here. Having distinct `user` and `assistant` text. Where the `user` prompt includes all conditioning of the model and the `assistant` text is the expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"Please perform a classification task. \"\n",
    "    \"Given the 2016 survey answers from the American National Election Studies, \"\n",
    "    \"return which candidate the person voted for. \"\n",
    "    \"Return a label from ['Trump', 'Clinton', 'Non-voter'] only without any other text.\\n\"\n",
    ")\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_map = {\n",
    "    \"race\": \"Race\",\n",
    "    \"discuss_politics\": \"Discusses politics\",\n",
    "    \"ideology\": \"Ideology\",\n",
    "    \"party\": \"Party\",\n",
    "    \"church_goer\": \"Church\",\n",
    "    \"age\": \"Age\",\n",
    "    \"gender\": \"Gender\",\n",
    "    \"political_interest\": \"Political interest\",\n",
    "    \"patriotism\": \"American Flag\",\n",
    "    \"state\": \"State\",\n",
    "    \"ground_truth\": \"Vote\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_prompt_completion(\n",
    "    row: dict,\n",
    "    system_prompt: str = instruction,\n",
    ") -> list[list[dict]]:\n",
    "    user_prompt = \"\\n\".join(\n",
    "        [f\"{column_name_map[k]}: {v}\" for k, v in row.items() if k != label]\n",
    "    )\n",
    "    assistant_prompt = row[label]\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            },\n",
    "        ],\n",
    "        \"completion\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "build_prompt_completion(\n",
    "    row=dataset[\"train\"][0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_llm = dataset.map(build_prompt_completion).remove_columns(features+[label])\n",
    "dataset_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "We use a very small (1B parameters) model here for demonstration proposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model selection\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # revision=revision, # NOTE: revision should be set for an reproducible experiment \n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Quantization reduces the memory required to store the model (Dettmers et al., 2022). Typically a model is stored in 16-bit precision, therefore for a 70B parameter model:\n",
    "\n",
    "$$\\frac{16 \\text{ bits}}{8 \\text{ bits/byte}} \\times 70 \\times 10^9 \\text{ parameters} = 140 \\text{ GB of VRAM}$$\n",
    "\n",
    "With 4-bit quantization, all parameters are stored in 4-bit precision, reducing the memory requirement to:\n",
    "\n",
    "$$\\frac{4 \\text{ bits}}{8 \\text{ bits/byte}} \\times 70 \\times 10^9 \\text{ parameters} = 35 \\text{ GB of VRAM}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model in 4bit\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "    ),\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA\n",
    "Low-Rank Adapters (LoRA) are a parameter efficient fine-tuning method (Hu et al., 2021). Instead of finetuning all model weights, LoRA finetunes the weights of the adapter layers only. This requires less memory and allows for faster finetuning.\n",
    "\n",
    "![LoRA Diagram](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "For simplicity we evaluate on the _first token_ of the LLM generated answer only. If the first token with the highest probability matches the true first token we have classified correctly. We therefore can use typical classification metrics, i.e., _Accuracy_ and _F1 score_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "hf_metrics = [\n",
    "    evaluate.load(\"accuracy\"),\n",
    "    # evaluate.load(\"f1\"),\n",
    "]\n",
    "\n",
    "for metric in hf_metrics:\n",
    "    print(metric.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Training Setup\n",
    "This is python code is not covered in the workshop and such sets up some additional details for fine-tuning and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# set pad token id\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "if getattr(model.config, \"pad_token_id\") is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "labels = df_survey_processed.ground_truth.unique()\n",
    "\n",
    "first_token_ids = [\n",
    "    tokenizer.encode(label, add_special_tokens=False)[0]\n",
    "    for label in labels\n",
    "]\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    # Return highest probability token in answer options\n",
    "    # return logits[:, :, first_token_ids]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "y_probas = []\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    labels = labels[:, 1:]\n",
    "    preds = preds[:, :-1]\n",
    "\n",
    "    # -100 is a default value for ignore_index used by DataCollatorForCompletionOnlyLM\n",
    "    mask = labels == -100\n",
    "    labels[mask] = tokenizer.pad_token_id\n",
    "    preds[mask] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    results = {}\n",
    "    for metric in hf_metrics:\n",
    "        results |= metric.compute(predictions=preds[~mask], references=labels[~mask])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "To run the training without wandb logging, set `wandb.init(mode='disabled')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from datetime import datetime\n",
    "\n",
    "# key hyperparameters\n",
    "learning_rate = 2e-5\n",
    "batch_size = 8\n",
    "epochs = 1 # NOTE you should run multiple epochs in practice\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "dataset_name = \"argyle_anes_2016\"\n",
    "run_name = f\"{model_id}_{dataset_name}_seed_{seed}_{now}\"\n",
    "\n",
    "# wandb.init(\n",
    "#     mode='disabled',\n",
    "# )\n",
    "wandb.init(\n",
    "    project=\"aapor-finetuning\",\n",
    "    name=run_name,\n",
    ")\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # training\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs, \n",
    "\n",
    "    # evaluation\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1 / 3,  # after each third of an epoch\n",
    "\n",
    "    # logging\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name,\n",
    "\n",
    "    output_dir=\"./results\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_llm[\"train\"],\n",
    "    eval_dataset=dataset_llm[\"test\"],\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fine-tuning results can be found live at: https://wandb.ai/tobihol/aapor-finetuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
