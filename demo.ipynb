{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Description of Task\n",
    "\n",
    "we wan to fine-tune a model for for vote choice prediction.\n",
    "\n",
    "For the exercises the `TODO` marks places where the the answer should be written/coded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup\n",
    "The following is the technical setup required to run the exercises 1, 2, and 3 content, when running the notebook on Google Colab or Kaggle. \n",
    "\n",
    "For the fine-tuning we have additional technical setup later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional colab/kaggle setup\n",
    "\n",
    "# Importing the sys module to access system-specific parameters and functions\n",
    "import sys\n",
    "# Importing the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Downloads the data from the GitHub repository of this notebook\n",
    "def download_data():\n",
    "    !git clone https://github.com/tobihol/aapor-finetuning.git\n",
    "    %mv aapor-finetuning/data/ .\n",
    "    %rm -rf aapor-finetuning/\n",
    "    return\n",
    "\n",
    "# Checks if the code is running in a Kaggle environment\n",
    "def is_running_in_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "# Checks if the code is running in a Google Colab environment\n",
    "def is_running_in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "# Check if running in Colab or Kaggle and download data accordingly\n",
    "if is_running_in_colab() or is_running_in_kaggle():\n",
    "    print(\"Running on Colab/Kaggle\")\n",
    "    download_data()\n",
    "else:\n",
    "    print(\"Not running in Colab/Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Model Selection\n",
    "\n",
    "Choose the right model based on your use case.\n",
    "\n",
    "Before selecting a model for your task, it’s important to reflect on your goals, resources, and technical environment. Here are key questions to guide your decision:\n",
    "\n",
    "Do you want to run the model on your own hardware?\n",
    "- Open-source models (like LLaMA, Mistral, or Qwen) can often be downloaded and run locally. This gives you more control over data privacy and fine-tuning, but also requires sufficient computing power (especially GPU memory) and setup effort.\n",
    "\n",
    "Do you have funds to use a proprietary model?\n",
    "- Commercial models like OpenAI’s GPT-4, Anthropic’s Claude, or Google’s Gemini offer strong performance and easy-to-use APIs, but they come with usage costs. This can be worthwhile if you want strong performance without managing infrastructure.\n",
    "\n",
    "Do you have strong programming skills?\n",
    "- Open-source models typically require setting up environments (e.g., using PyTorch, HuggingFace Transformers), handling tokenization, managing memory usage, and sometimes fine-tuning. If you’re comfortable with Python and command-line tools, these are manageable. If not, hosted APIs might be easier to work with.\n",
    "\n",
    "Think about your use case and pick a model. \n",
    "\n",
    "**Resources**\n",
    "\n",
    "The following resources give an overview of available models and how they compare on different metrics of performance and cost.\n",
    "\n",
    "- The popular benchmark **Chatbot Arena** _(Chiang et al., 2024)_  provides a live updated elo score based on crowd-sourced pairwise comparisons of their responses.\n",
    "    - Trade-off Plot: https://lmarena.ai/price\n",
    "- The **Stanford HELM Benchmark** evaluates language models on over 50 tasks, including understanding and reasoning. It provides insights into model performance in terms of accuracy, fairness, and robustness.\n",
    "    - https://crfm.stanford.edu/helm/\n",
    "- The **Artificial Analysis** platform provides independent analysis of AI models and API providers. The main metrics are 'intelligence', speed, and price. \n",
    "    - https://artificialanalysis.ai/\n",
    "- The **Open LLM Leaderboard** on Hugging Face runs multiple well established benchmarks on for open-source models hosted on the site. It is most useful to compare smaller open-source LLMs that are not featured on Chatbot Arena or compare different fine-tunes of models. \n",
    "    - https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "Feel free to make notes here, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we are picking**\n",
    "\n",
    "Our use case is to predict vote choices in the US presidential election based on ANES survey answer. We want to fine-tune a model for _free_ and therefore do not have a lot of computing resources (Google Colab free tier gives access to one T4 GPU). We therefor use the smallest model in the Llama 3 family, **Llama 3.2 1B**, as our model of choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Excel to JSON\n",
    "\n",
    "Typical data that is processed by LLMs is assumed to be in JSON format. JSON is a standardized, lightweight, and language-independent format that allows structured data—like text, numbers, lists, or nested objects—to be easily transmitted between systems. Most APIs for LLMs expect JSON because it's both human-readable and machine-readable, making it ideal for sending configuration settings, user input, or model instructions in a clear and organized way.\n",
    "\n",
    "To get a feeling for how data is structured in JSON as opposed to tabular data (Excel, CSV) we want to convert a small table to JSON in this task.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Excel participants table:\n",
    "|   Age | Gender | Education | Occupation |\n",
    "|------:|:-------|:----------|:-----------|\n",
    "|    30 | male   | bachelor  | accountant |\n",
    "|    24 | female | master    | engineer   |\n",
    "\n",
    "Converted to JSON:\n",
    "```json\n",
    "{\n",
    "    \"participants\": [\n",
    "        {\n",
    "            \"age\": 30,\n",
    "            \"gender\": \"male\",\n",
    "            \"education\": \"bachelor\",\n",
    "            \"occupation\": \"accountant\"\n",
    "        },\n",
    "        {\n",
    "            \"age\": 24,\n",
    "            \"gender\": \"female\",\n",
    "            \"education\": \"master\",\n",
    "            \"occupation\": \"engineer\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task**\n",
    "\n",
    "Given the following table of participant answers in a survey:\n",
    "\n",
    "|   Age | Gender   | State     | Vote Choice   |\n",
    "|------:|:---------|:----------|:--------------|\n",
    "|    27 | female   | Louisiana | Trump         |\n",
    "|    45 | male     | NaN       | Clinton       |\n",
    "|   NaN | female   | Ohio      | Non-voter     |\n",
    "\n",
    "What would a corresponding JSON entry look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type up your answer here\n",
    "\n",
    "json_data = {\n",
    "    \"participants\": [\n",
    "        # TODO\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is done in practice**\n",
    "\n",
    "Of course tables are not manually converted in to JSON data in practice Typically python libraries are used to handle data manipulation tasks. In this short course we will use the popular library _pandas_ for parsing tabular data. In the following, two ways of transforming Excel data into JSON data are shown using the library.\n",
    "\n",
    "Records orientation\n",
    "- Each row of the table is converted into a JSON object, with the column names as keys and the row values as the corresponding values. This results in a list of JSON objects, where each object represents a row in the table.\n",
    "\n",
    "Columns orientation\n",
    "- The table is converted into a JSON object where each key is a column name, and the value is an object containing the index-value pairs for that column. This orientation is useful when you want to access data by columns rather than by rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the popular data manipulation library pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file into a DataFrame, using the first column as the index\n",
    "data = pd.read_excel(\"data/json_task.xlsx\", index_col=0)\n",
    "\n",
    "# Convert the DataFrame to JSON using records orientation\n",
    "# Each row is converted into a JSON object with column names as keys\n",
    "print(\"--- Records Orientation ---\")\n",
    "print(data.to_json(orient=\"records\"))\n",
    "print()\n",
    "\n",
    "# Convert the DataFrame to JSON using columns orientation\n",
    "# Each column is converted into a JSON object with index-value pairs\n",
    "print(\"--- Columns Orientation ---\")\n",
    "print(data.to_json(orient=\"columns\"))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Prompt Design\n",
    "\n",
    "In this exercise, we want to convert our JSON survey participant data, into the structure we will give to the LLM. We will use an _instruction-tuned_ LLM, as they are specifically trained to follow human instructions, as opposed to base models, who predict the next token in a continuous text. Instruction-tuned models use a _chat-format_ as input. These _instruction-tuned_ LLMs follow often follow a similar prompt format, described in the following.\n",
    "\n",
    "**The OpenAI Chat Format**\n",
    "\n",
    "The OpenAI JSON chat format has become the standard way to structure conversations with AI models. This format is used not only by OpenAI's models but also by many open-source models available on Hugging Face. This standardized format makes it easy to switch between different models (OpenAI, open-source models on Hugging Face, etc.), maintain conversation history in a structured way, and control the behavior of the model through system messages.\n",
    "\n",
    "The format consists of a list of messages, each with a \"role\" and \"content\":\n",
    "- **role**: Identifies who is speaking (system, user, or assistant)\n",
    "- **content**: Contains the actual message text\n",
    "\n",
    "Overview of the three roles:\n",
    "- The **system** role provides initial instructions to the model. It sets the behavior and context for the AI assistant. It defines the assistant's role, capabilities, and constraints. The system message not shown to the end user, when using chat bots like ChatGPT, but guides how the model responds.\n",
    "- The **user** role represents inputs from the human.\n",
    "- The **assistant** role contains the model's responses.\n",
    "\n",
    "**Example Conversion**\n",
    "\n",
    "An example of how to construct a prompt based on survey data in JSON. Here we only consider one participant to make the example more manageable.\n",
    "\n",
    "Survey data:\n",
    "```json\n",
    "{\n",
    "    \"participants\": [\n",
    "        {\n",
    "            \"age\": 30,\n",
    "            \"gender\": \"male\",\n",
    "            \"education\": \"bachelor\",\n",
    "            \"occupation\": \"accountant\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "An example prompting setup where we want the LLM to be a survey participant that answers questions.\n",
    "```json\n",
    "{\n",
    "    \"prompt\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a survey participant. Reply to the user's question with a short answer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your age?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am 30 years old.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your gender?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am a male.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your occupation?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am an accountant.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task**\n",
    "\n",
    "Using the same survey data as provided in the example above. Create a prompting setup that tasks the LLM with predicting the vote choice of a the participant instead.\n",
    "\n",
    "Survey data:\n",
    "```json\n",
    "{\n",
    "    \"participants\": [\n",
    "        {\n",
    "            \"age\": 30,\n",
    "            \"gender\": \"male\",\n",
    "            \"education\": \"bachelor\",\n",
    "            \"occupation\": \"accountant\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Questions to consider:**\n",
    "- What should the assitant answer look like?\n",
    "- Should the model act as an expert in a certain field? Who does it substitute?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type up your answer here\n",
    "\n",
    "json_prompt = {\n",
    "    \"prompt\": [\n",
    "        # TODO\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using our prompt with an LLM**\n",
    "\n",
    "We can test our prompt with a small open-source model. To do this do _not_ include the \"assistant\" answer JSON object you want the LLM to answer in the prompt. The assistant answer will instead be generated by the LLM, which we choose back in Exercise 1 and we use the instruction-tuned version of the model, as described earlier in this exercise. \n",
    "\n",
    "We load the model _Llama-3.2-1B-Instruct_ with the `transformers` library. The transformers library is a popular open-source library developed by Hugging Face that provides pre-trained models for natural language processing (NLP) and other machine learning tasks. It includes tools for fine-tuning, inference optimization, and model deployment. In this notebook, we'll use transformers to load and run the Llama-3.2-1B-Instruct model for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pipeline module from the transformers library\n",
    "# This provides a simple way to use pre-trained models for various NLP tasks\n",
    "# such as text generation, without having to manually handle model loading and inference\n",
    "from transformers import pipeline\n",
    "\n",
    "# id of a model hosted on Hugging Face\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"  # NOTE: we use the unsloth version of the model, because it does not require an API key, as opposed to the official meta-llama version\n",
    "\n",
    "# Create a pipeline for an instruct model using the json_prompt as input\n",
    "instruct_pipeline = pipeline(\n",
    "    \"text-generation\",  # Specifies the task type for the pipeline (generating text)\n",
    "    model=model_id,     # Sets which model to use (Llama-3.2-1B-Instruct in this case)\n",
    "    device_map=\"auto\",  # Automatically determines the best device (CPU/GPU) for running the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the model, we now generate 10 responses to our prompt `json_prompt`. The responses can differ, as they are sampled probabilistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response using the instruct model pipeline\n",
    "responses = instruct_pipeline(json_prompt[\"prompt\"], num_return_sequences=10)\n",
    "\n",
    "# Print the generated response\n",
    "print(\"10 different generated responses:\")\n",
    "[resp[\"generated_text\"][-1] for resp in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Pipeline\n",
    "In the following we go through the steps required to setup a fine-tuning pipeline to impute missing survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional colab/kaggle setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_dependencies():\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "      print(\"CUDA is not available. \\nPick a GPU before running this notebook. \\nGo to 'Runtime' -> 'Change runtime type' to do this. (Colab)\")\n",
    "      return \n",
    "    %pip install bitsandbytes\n",
    "    %pip install accelerate\n",
    "    %pip install transformers\n",
    "    %pip install datasets\n",
    "    %pip install evaluate\n",
    "    %pip install peft\n",
    "    %pip install trl\n",
    "    %pip install evaluate\n",
    "    %pip install scikit-learn\n",
    "    %pip install wandb\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def is_running_in_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "def is_running_in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "if is_running_in_colab() or is_running_in_kaggle():\n",
    "    print(\"Running on Colab/Kaggle\")\n",
    "    install_dependencies()\n",
    "else:\n",
    "    print(\"Not running in Colab/Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "seed = 24 # Please set your own favorite seed!\n",
    "transformers.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation\n",
    "We use 2016 American National Election Studies survey data. Specifically, the subset of data Argyle et al. (2022) used in study 2 (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JPV20K)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey = pd.read_csv(\"data/2016_anes_argyle.csv\")\n",
    "df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null values and data types\n",
    "df_survey.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"race\",\n",
    "    \"discuss_politics\",\n",
    "    \"ideology\",\n",
    "    \"party\",\n",
    "    \"church_goer\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"political_interest\",\n",
    "    \"patriotism\",\n",
    "    \"state\",\n",
    "]\n",
    "label = \"ground_truth\" # this is the vote choice in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tread missing values as a category \n",
    "df_survey_processed = (\n",
    "    df_survey\n",
    "    .astype({\"age\": str})\n",
    "    .fillna(\"missing\")\n",
    ")\n",
    "df_survey_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split\n",
    "\n",
    "Any manipulation of the training data should be done in the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_survey_processed, test_size=0.2, random_state=seed)\n",
    "\n",
    "# we can modify the training data here to do different experiments\n",
    "# for example excluding republican voters\n",
    "# leans_republican = df_train[\"party\"].apply(lambda x: \"Republican\" in x)\n",
    "# df_train = df_train[~leans_republican]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(df_test, preserve_index=False),\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Design\n",
    "\n",
    "We will use an instruction-tuned model and will therefore define an instruction prompt here. Having distinct `user` and `assistant` text. Where the `user` prompt includes all conditioning of the model and the `assistant` text is the expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"Please perform a classification task. \"\n",
    "    \"Given the 2016 survey answers from the American National Election Studies, \"\n",
    "    \"return which candidate the person voted for. \"\n",
    "    \"Return a label from ['Trump', 'Clinton', 'Non-voter'] only without any other text.\\n\"\n",
    ")\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_map = {\n",
    "    \"race\": \"Race\",\n",
    "    \"discuss_politics\": \"Discusses politics\",\n",
    "    \"ideology\": \"Ideology\",\n",
    "    \"party\": \"Party\",\n",
    "    \"church_goer\": \"Church\",\n",
    "    \"age\": \"Age\",\n",
    "    \"gender\": \"Gender\",\n",
    "    \"political_interest\": \"Political interest\",\n",
    "    \"patriotism\": \"American Flag\",\n",
    "    \"state\": \"State\",\n",
    "    \"ground_truth\": \"Vote\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_prompt_completion(\n",
    "    row: dict,\n",
    "    system_prompt: str = instruction,\n",
    ") -> list[list[dict]]:\n",
    "    user_prompt = \"\\n\".join(\n",
    "        [f\"{column_name_map[k]}: {v}\" for k, v in row.items() if k != label]\n",
    "    )\n",
    "    assistant_prompt = row[label]\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            },\n",
    "        ],\n",
    "        \"completion\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "build_prompt_completion(\n",
    "    row=dataset[\"train\"][0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_llm = dataset.map(build_prompt_completion).remove_columns(features+[label])\n",
    "dataset_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "We use a very small (1B parameters) model here for demonstration proposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model selection\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # revision=revision, # NOTE: revision should be set for an reproducible experiment \n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Quantization reduces the memory required to store the model (Dettmers et al., 2022). Typically a model is stored in 16-bit precision, therefore for a 70B parameter model:\n",
    "\n",
    "$$\\frac{16 \\text{ bits}}{8 \\text{ bits/byte}} \\times 70 \\times 10^9 \\text{ parameters} = 140 \\text{ GB of VRAM}$$\n",
    "\n",
    "With 4-bit quantization, all parameters are stored in 4-bit precision, reducing the memory requirement to:\n",
    "\n",
    "$$\\frac{4 \\text{ bits}}{8 \\text{ bits/byte}} \\times 70 \\times 10^9 \\text{ parameters} = 35 \\text{ GB of VRAM}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model in 4bit\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "    ),\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA\n",
    "Low-Rank Adapters (LoRA) are a parameter efficient fine-tuning method (Hu et al., 2021). Instead of finetuning all model weights, LoRA finetunes the weights of the adapter layers only. This requires less memory and allows for faster finetuning.\n",
    "\n",
    "![LoRA Diagram](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "For simplicity we evaluate on the _first token_ of the LLM generated answer only. If the first token with the highest probability matches the true first token we have classified correctly. We therefore can use typical classification metrics, i.e., _Accuracy_ and _F1 score_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "hf_metrics = [\n",
    "    evaluate.load(\"accuracy\"),\n",
    "    # evaluate.load(\"f1\"),\n",
    "]\n",
    "\n",
    "for metric in hf_metrics:\n",
    "    print(metric.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Training Setup\n",
    "This is python code is not covered in the workshop and such sets up some additional details for fine-tuning and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# set pad token id\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "if getattr(model.config, \"pad_token_id\") is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "labels = df_survey_processed.ground_truth.unique()\n",
    "\n",
    "first_token_ids = [\n",
    "    tokenizer.encode(label, add_special_tokens=False)[0]\n",
    "    for label in labels\n",
    "]\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    # Return highest probability token in answer options\n",
    "    # return logits[:, :, first_token_ids]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "y_probas = []\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    labels = labels[:, 1:]\n",
    "    preds = preds[:, :-1]\n",
    "\n",
    "    # -100 is a default value for ignore_index used by DataCollatorForCompletionOnlyLM\n",
    "    mask = labels == -100\n",
    "    labels[mask] = tokenizer.pad_token_id\n",
    "    preds[mask] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    results = {}\n",
    "    for metric in hf_metrics:\n",
    "        results |= metric.compute(predictions=preds[~mask], references=labels[~mask])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "To run the training without wandb logging, set `wandb.init(mode='disabled')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from datetime import datetime\n",
    "\n",
    "# key hyperparameters\n",
    "learning_rate = 2e-5\n",
    "batch_size = 8\n",
    "epochs = 1 # NOTE you should run multiple epochs in practice\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "dataset_name = \"argyle_anes_2016\"\n",
    "run_name = f\"{model_id}_{dataset_name}_seed_{seed}_{now}\"\n",
    "\n",
    "# wandb.init(\n",
    "#     mode='disabled',\n",
    "# )\n",
    "wandb.init(\n",
    "    project=\"aapor-finetuning\",\n",
    "    name=run_name,\n",
    ")\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # training\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs, \n",
    "\n",
    "    # evaluation\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1 / 3,  # after each third of an epoch\n",
    "\n",
    "    # logging\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name,\n",
    "\n",
    "    output_dir=\"./results\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_llm[\"train\"],\n",
    "    eval_dataset=dataset_llm[\"test\"],\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fine-tuning results can be found live at: https://wandb.ai/tobihol/aapor-finetuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
