{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional google colab setup\n",
    "import sys\n",
    "\n",
    "\n",
    "def colab_install():\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "      print(\"CUDA is not available. \\nPick a GPU before running this notebook. \\nGo to 'Runtime' -> 'Change runtime type' to do this.\")\n",
    "      return \n",
    "    %pip install transformers\n",
    "    %pip install datasets\n",
    "    %pip install peft\n",
    "    %pip install bitsandbytes\n",
    "    return\n",
    "\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Install required packages\n",
    "    colab_install()\n",
    "else:\n",
    "    print(\"Not running in Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "transformers.set_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    %cd survai-finetuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "!curl -L -o 2016_anes_argyle.pkl https://github.com/tobihol/survai-finetuning/raw/main/2016_anes_argyle.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey = pd.read_pickle(\"2016_anes_argyle.pkl\")\n",
    "df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics\n",
    "df_survey.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"race\",\n",
    "    \"discuss_politics\",\n",
    "    \"ideology\",\n",
    "    \"party\",\n",
    "    \"church_goer\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"political_interest\",\n",
    "    \"patriotism\",\n",
    "    \"state\",\n",
    "]\n",
    "label = \"ground_truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tread missing values as a category \n",
    "df_survey_processed = (\n",
    "    df_survey\n",
    "    .astype({\"age\": str})\n",
    "    .fillna(\"missing\")\n",
    ")\n",
    "df_survey_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"Please perform a classification task. \"\n",
    "    + \"Given the 2016 survey answers from the American National Election Studies, \"\n",
    "    + \"return which candiate the person voted for. \"\n",
    "    + \"Return a label from ['Trump', 'Clinton', 'Non-voter'] only without any other text.\\n\"\n",
    ")\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_map = {\n",
    "    \"race\": \"Race\",\n",
    "    \"discuss_politics\": \"Discusses politics\",\n",
    "    \"ideology\": \"Ideology\",\n",
    "    \"party\": \"Party\",\n",
    "    \"church_goer\": \"Church\",\n",
    "    \"age\": \"Age\",\n",
    "    \"gender\": \"Gender\",\n",
    "    \"political_interest\": \"Political interest\",\n",
    "    \"patriotism\": \"American Flag\",\n",
    "    \"state\": \"State\",\n",
    "    \"ground_truth\": \"Vote\",\n",
    "}\n",
    "\n",
    "def create_prompt(row):\n",
    "    prompt = instruction\n",
    "    prompt += \"\\n\".join([f\"{column_name_map[k]}: {v}\" for k, v in row.items()])\n",
    "    return prompt\n",
    "\n",
    "text_series = df_survey_processed[features].apply(create_prompt, axis=\"columns\")\n",
    "label_series = df_survey_processed[label]\n",
    "\n",
    "df_prompts = pd.DataFrame({\"text\": text_series, \"label\": label_series})\n",
    "df_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prompts.text.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prompts.label.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_survey_processed, test_size=0.2, random_state=24)\n",
    "dataset_cls = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(df_test, preserve_index=False),\n",
    "})\n",
    "dataset_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_prompts, test_size=0.2, random_state=24)\n",
    "dataset_llm = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(df_test, preserve_index=False),\n",
    "})\n",
    "dataset_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"EleutherAI/pythia-70m\"\n",
    "# model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# dataset = dataset.map(\n",
    "#     lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), batched=True\n",
    "# )\n",
    "\n",
    "# dataset = dataset.remove_columns([\"text\", \"label\"])\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruct_tokenize_function(examples):\n",
    "    prompt = [\n",
    "        {\"role\": \"user\", \"content\": examples[\"text\"]},\n",
    "    ]\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": examples[\"label\"],\n",
    "        }\n",
    "    )\n",
    "    inputs_ids = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    attention_mask = np.ones_like(inputs_ids)\n",
    "    return {\n",
    "        \"input_ids\": inputs_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "def basic_tokenize_function(examples):\n",
    "    prompt = f\"{examples['text']} \\nVote: {examples['label']} {tokenizer.eos_token}\"\n",
    "    return tokenizer(prompt)\n",
    "\n",
    "\n",
    "tokenized_dataset_llm = dataset_llm.map(basic_tokenize_function).remove_columns(\n",
    "    [\"text\", \"label\"]\n",
    ")\n",
    "tokenized_dataset_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "if getattr(model.config, \"pad_token_id\") is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset_llm[\"train\"],\n",
    "    eval_dataset=tokenized_dataset_llm[\"test\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (survai-finetuning)",
   "language": "python",
   "name": "survai-finetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
