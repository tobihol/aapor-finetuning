{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional google colab setup\n",
    "import sys\n",
    "\n",
    "\n",
    "def colab_install():\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "      print(\"CUDA is not available. \\nPick a GPU before running this notebook. \\nGo to 'Runtime' -> 'Change runtime type' to do this.\")\n",
    "      return \n",
    "    %pip install numpy==1.* # ligtheval is not compatible with 2.0 TODO: check this\n",
    "    %pip install lighteval\n",
    "    %pip install transformers\n",
    "    %pip install datasets\n",
    "    %pip install peft\n",
    "    %pip install bitsandbytes\n",
    "    %pip install evaluate\n",
    "    %pip install wandb\n",
    "    return\n",
    "\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Install required packages\n",
    "    colab_install()\n",
    "else:\n",
    "    print(\"Not running in Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "transformers.set_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    %cd survai-finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "!curl -L -o 2016_anes_argyle.pkl https://github.com/tobihol/survai-finetuning/raw/main/2016_anes_argyle.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey = pd.read_pickle(\"2016_anes_argyle.pkl\")\n",
    "df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics\n",
    "df_survey.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"race\",\n",
    "    \"discuss_politics\",\n",
    "    \"ideology\",\n",
    "    \"party\",\n",
    "    \"church_goer\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"political_interest\",\n",
    "    \"patriotism\",\n",
    "    \"state\",\n",
    "]\n",
    "label = \"ground_truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tread missing values as a category \n",
    "df_survey_processed = (\n",
    "    df_survey\n",
    "    .astype({\"age\": str})\n",
    "    .fillna(\"missing\")\n",
    ")\n",
    "df_survey_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_survey_processed, test_size=0.2, random_state=24)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(df_test, preserve_index=False),\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"Please perform a classification task. \"\n",
    "    + \"Given the 2016 survey answers from the American National Election Studies, \"\n",
    "    + \"return which candiate the person voted for. \"\n",
    "    + \"Return a label from ['Trump', 'Clinton', 'Non-voter'] only without any other text.\\n\"\n",
    ")\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_map = {\n",
    "    \"race\": \"Race\",\n",
    "    \"discuss_politics\": \"Discusses politics\",\n",
    "    \"ideology\": \"Ideology\",\n",
    "    \"party\": \"Party\",\n",
    "    \"church_goer\": \"Church\",\n",
    "    \"age\": \"Age\",\n",
    "    \"gender\": \"Gender\",\n",
    "    \"political_interest\": \"Political interest\",\n",
    "    \"patriotism\": \"American Flag\",\n",
    "    \"state\": \"State\",\n",
    "    \"ground_truth\": \"Vote\",\n",
    "}\n",
    "\n",
    "def create_prompt(row):\n",
    "    prompt = instruction\n",
    "    prompt += \"\\n\".join([f\"{column_name_map[k]}: {v}\" for k, v in row.items()])\n",
    "    return prompt\n",
    "\n",
    "def map_to_prompt(row):\n",
    "    user_prompt = instruction\n",
    "    user_prompt += \"\\n\".join([f\"{column_name_map[k]}: {v}\" for k, v in row.items() if k != label])\n",
    "    assistant_prompt = row[label]\n",
    "    return {\n",
    "        \"text\": user_prompt, \n",
    "        \"label\": assistant_prompt,\n",
    "        }\n",
    "\n",
    "map_to_prompt(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_llm = dataset.map(map_to_prompt).remove_columns(features+[label])\n",
    "dataset_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM training/inference is the wild west: the are a ton of differnt libraries/wrappers where each one can implement different changes your evaluations results. These libaries also get often fixed and updated in major ways which can break your pipeline or change results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "### Which modle should I fine-tune? \n",
    "State-of-the-Art open-source model: **Llama 3 model family** *(Dubey et al., 2024)*\n",
    "- Best performance, use full for testing the best possible performance to data\n",
    "\n",
    "Research model: **Pythia model family** *(Biderman et al., 2023)*\n",
    "- Openly available training data\n",
    "- Multiple smaller model sizes available\n",
    "- Enables testing your finetuning pipeline more efficiently\n",
    "- Enables comparing the effects of model size on performance\n",
    "- Easy to test for data contamination\n",
    "- Drawback: May not give a good representation of what is possible with SOTA models\n",
    "\n",
    "\n",
    "\n",
    "### Which models currently perform best?\n",
    "- https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n",
    "- https://lmarena.ai/\n",
    "- https://crfm.stanford.edu/helm/\n",
    "    - Imputation Benchmark: https://crfm.stanford.edu/helm/classic/latest/#/groups/entity_data_imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"EleutherAI/pythia-70m\"\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "# revision = \"8d308458221c84f2b793d9b820d72e2c10159630\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # revision=revision,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem I encounted during my pipeline implementations using: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this if you don't have a huggingface account\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello world\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello\"},\n",
    "]\n",
    "\n",
    "tokenizer_mistral_old = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    revision=\"41b61a33a2483885c981aa79e0df6b32407ed873\",\n",
    ")\n",
    "\n",
    "untokenized_output_mistral_old = tokenizer_mistral_old.apply_chat_template(\n",
    "    chat,\n",
    "    tokenize=False,\n",
    ")\n",
    "print(f\"Untokenized output: {untokenized_output_mistral_old}\")\n",
    "\n",
    "tokenized_output_mistral_old = tokenizer_mistral_old.apply_chat_template(\n",
    "    chat,\n",
    "    tokenize=True,\n",
    ")\n",
    "print(f\"Tokenized output: {tokenized_output_mistral_old}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this if you don't have a huggingface account\n",
    "tokenizer_mistral_new = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\", revision=\"main\"\n",
    ")\n",
    "\n",
    "untokenized_output_mistral_new = tokenizer_mistral_new.apply_chat_template(\n",
    "    chat,\n",
    "    tokenize=False,\n",
    ")\n",
    "print(f\"Untokenized output: {untokenized_output_mistral_new}\")\n",
    "\n",
    "tokenized_output_mistral_new = tokenizer_mistral_new.apply_chat_template(\n",
    "    chat,\n",
    "    tokenize=True,\n",
    ")\n",
    "print(f\"Tokenized output: {tokenized_output_mistral_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> The token for the `Hello` answer of the assitant is different!\n",
    "\n",
    "Not only dependency versions should be reported, but also the model version! As even small changes in the tokenizer can cause major changes in the output and make a finding not reproducable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruct_tokenize_function(examples):\n",
    "    prompt = [\n",
    "        {\"role\": \"user\", \"content\": examples[\"text\"]},\n",
    "    ]\n",
    "    prompt.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": examples[\"label\"],\n",
    "        }\n",
    "    )\n",
    "    inputs_ids = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    attention_mask = np.ones_like(inputs_ids)\n",
    "    return {\n",
    "        \"input_ids\": inputs_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "def basic_tokenize_function(examples):\n",
    "    prompt = f\"{examples['text']} \\nVote: {examples['label']} {tokenizer.eos_token}\"\n",
    "    return tokenizer(prompt)\n",
    "\n",
    "\n",
    "tokenized_dataset_llm = dataset_llm.map(basic_tokenize_function).remove_columns(\n",
    "    [\"text\", \"label\"]\n",
    ")\n",
    "tokenized_dataset_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Quantization reduces the memory required to store the model (Dettmers et al., 2022). Typically a model is stored in 16bit, therefor for a 70B model typically 16/8 bytes * 70 * 10^9 = 140GB of VRAM would be needed to store it. With 4bit quantisation all parameters are stored in 4bit and therefore only 4/8 bytes * 70 * 10^9 = 35GB of VRAM is needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model in 4bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "if getattr(model.config, \"pad_token_id\") is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA\n",
    "Low-Rank Adapters (LoRA) are a parameter efficient fine-tuning method (Hu et al., 2021). Instead of finetuning all model weights, LoRA finetunes the weights of the adapter layers only. This requires less memory and allows for faster finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Answer Extraction Problem\n",
    "\n",
    "<!-- - https://arxiv.org/pdf/2307.09702, https://github.com/dottxt-ai/outlines -->\n",
    "Different modles for answer extraction:\n",
    "- https://blog.eleuther.ai/multiple-choice-normalization/\n",
    "- https://github.com/huggingface/lighteval\n",
    "\n",
    "Problem 1: How many tokens are need to answer the question:\n",
    "- One token solutions:\n",
    "    - less compute intensive\n",
    "    - do not require normalisation\n",
    "    - only works if all first token are destinct\n",
    "- Multi token solutions:\n",
    "    - more compute intensive (multiplied by number of lables)\n",
    "    - might require normalisation\n",
    "    - does not require all first tokens to be distinct\n",
    "    \n",
    "Problem 2: How to evaluate multi token extraction (see code below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lighteval.metrics.metrics_sample import LoglikelihoodAcc\n",
    "from lighteval.metrics.normalizations import (\n",
    "    LogProbCharNorm,\n",
    "    # LogProbTokenNorm,\n",
    "    # LogProbPMINorm,\n",
    ")\n",
    "from lighteval.tasks.requests import Doc\n",
    "import numpy as np\n",
    "\n",
    "acc_metric = LoglikelihoodAcc(\n",
    "    # LogProbCharNorm(ignore_first_space=False),\n",
    ")\n",
    "\n",
    "choices = [\"Trump\", \"Clinton\", \"Non-voter\"]\n",
    "log_prob_predictions = np.log([0.34, 0.33, 0.32])\n",
    "correct_choice = \"Non-voter\"\n",
    "\n",
    "doc = Doc(query=\"...\", choices=choices, gold_index=[choices.index(correct_choice)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_without_normalisation = LoglikelihoodAcc(\n",
    "    # LogProbCharNorm(ignore_first_space=False),\n",
    ").compute(\n",
    "    gold_ixs=doc.gold_index,\n",
    "    choices_logprob=log_prob_predictions,\n",
    "    unconditioned_logprob=None,\n",
    "    choices_tokens=None,\n",
    "    formatted_doc=doc,\n",
    ")\n",
    "print(f\"Accuracy score without normalisation: {acc_without_normalisation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_with_normalisation = LoglikelihoodAcc(\n",
    "    LogProbCharNorm(ignore_first_space=False),\n",
    ").compute(\n",
    "    gold_ixs=doc.gold_index,\n",
    "    choices_logprob=log_prob_predictions,\n",
    "    unconditioned_logprob=None,\n",
    "    choices_tokens=None,\n",
    "    formatted_doc=doc,\n",
    ")\n",
    "print(f\"Accuracy score with normalisation: {acc_with_normalisation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "\n",
    "# TODO: make other metrics work\n",
    "hf_metrics = [\n",
    "    evaluate.load(\"accuracy\"),\n",
    "    # evaluate.load(\"f1\"),\n",
    "    # evaluate.load(\"precision\"),\n",
    "    # evaluate.load(\"recall\"),\n",
    "    # evaluate.load(\"confusion_matrix\"),\n",
    "]\n",
    "sklearn_metrics = {\n",
    "    # \"accuracy\": metrics.accuracy_score,\n",
    "    # \"balanced_accuracy\": metrics.balanced_accuracy_score,\n",
    "    # \"f1_weighted\": partial(metrics.f1_score, average=\"weighted\"),\n",
    "    # \"confusion_matrix\": metrics.confusion_matrix,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def instruct_tokenization(\n",
    "    data: DatasetDict,\n",
    "    tokenizer: AutoTokenizer,\n",
    ") -> Tuple[DatasetDict, Dataset]:\n",
    "    def tokenize_function(examples, is_inference=False):\n",
    "        prompt = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"text\"]},\n",
    "        ]\n",
    "        if not is_inference:\n",
    "            prompt.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": examples[\"label\"],\n",
    "                }\n",
    "            )\n",
    "        inputs_ids = tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            add_generation_prompt=is_inference,\n",
    "        )\n",
    "        attention_mask = np.ones_like(inputs_ids)\n",
    "        return {\n",
    "            \"input_ids\": inputs_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "    column_names = list(data.column_names.values())[0]\n",
    "    training_data = data.map(tokenize_function, remove_columns=column_names)\n",
    "    from functools import partial\n",
    "\n",
    "    inference_data = data.map(\n",
    "        partial(tokenize_function, is_inference=True), remove_columns=column_names\n",
    "    )\n",
    "\n",
    "    answer_tokens = list(\n",
    "        {\n",
    "            training_ids[len(inference_ids)]\n",
    "            for inference_ids, training_ids in zip(\n",
    "                inference_data[\"train\"][\"input_ids\"]\n",
    "                + inference_data[\"test\"][\"input_ids\"],\n",
    "                training_data[\"train\"][\"input_ids\"]\n",
    "                + training_data[\"test\"][\"input_ids\"],\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    assert len(answer_tokens) == len(\n",
    "        set(data[\"test\"][\"label\"] + data[\"train\"][\"label\"])\n",
    "    )\n",
    "\n",
    "    return training_data, inference_data, answer_tokens\n",
    "\n",
    "\n",
    "training_data, inference_data, answer_tokens = instruct_tokenization(\n",
    "    dataset_llm, tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_slice_ids = [\n",
    "    (len(inference_ids), len(training_ids) - 1)\n",
    "    for inference_ids, training_ids in zip(\n",
    "        inference_data[\"test\"][\"input_ids\"], training_data[\"test\"][\"input_ids\"]\n",
    "    )\n",
    "]  # NOTE: the -1 accounts for the eos token, which is not present for the generation data\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    logits = logits[:, :, answer_tokens].argmax(dim=-1)\n",
    "\n",
    "    return torch.tensor(\n",
    "        answer_tokens,\n",
    "        device=\"cuda\",\n",
    "    )[logits]\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    preds = np.pad(preds, ((0, 0), (1, 0)), mode=\"constant\", constant_values=-100)\n",
    "    labels = np.pad(labels, ((0, 0), (0, 1)), mode=\"constant\", constant_values=-100)\n",
    "\n",
    "    def get_slice(y):\n",
    "        return [\n",
    "            [token for token, label_token in zip(row, label) if label_token != -100][\n",
    "                start_id:end_id\n",
    "            ]\n",
    "            for (start_id, end_id), row, label in zip(pred_slice_ids, y, labels)\n",
    "        ]\n",
    "\n",
    "    y_true = get_slice(labels)\n",
    "    y_pred = get_slice(preds)\n",
    "    # accuracy based on the first token of the vote\n",
    "    y_true = [row[0] for row in y_true]\n",
    "    y_pred = [row[0] for row in y_pred]\n",
    "\n",
    "    results = {}\n",
    "    for metric in hf_metrics:\n",
    "        results |= metric.compute(predictions=y_pred, references=y_true)\n",
    "    for metric_name, metric_func in sklearn_metrics.items():\n",
    "        results[metric_name] = metric_func(y_true=y_true, y_pred=y_pred)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name = f\"{model_id}_{now}\"\n",
    "\n",
    "\n",
    "def finetune(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    training_data,\n",
    "    run_name,\n",
    "):\n",
    "    # wandb.init(\n",
    "    #     mode='disabled',\n",
    "    # )\n",
    "    wandb.init(\n",
    "        project=\"survai-finetuning\",\n",
    "        name=run_name,\n",
    "    )\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=training_data[\"train\"],\n",
    "        eval_dataset=training_data[\"test\"],\n",
    "        args=transformers.TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            gradient_checkpointing=True,\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "            fp16=True,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "\n",
    "            # train/eval settings\n",
    "            num_train_epochs=1,\n",
    "            do_eval=True,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=1 / 3,  # after each third\n",
    "\n",
    "            # logging\n",
    "            logging_steps=10,\n",
    "            report_to=\"wandb\",\n",
    "            run_name=run_name,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "            tokenizer, mlm=False\n",
    "        ),\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.evaluate()\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic Non-responses Experiment\n",
    "\n",
    "The party affiliation is (obviously) a strong predictor of vote choice. In the Argyle et al. (2022) study, the GPT-3 mainly used the party affiliation and ideology of a person to predict the vote choice.\n",
    "\n",
    "In this experiment we remove Repulican voters from the train set. We therefore only train on democrats and independents and see if the model can still perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"party\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leans_republican = df_train[\"party\"].apply(lambda x: \"Republican\" in x)\n",
    "df_train_ex2 = df_train[~leans_republican]\n",
    "df_train_ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO probably rerunning the notebook would be the right way to go about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things we did not cover\n",
    "\n",
    "Some parts of the pipeline we did not do, because of time constraints, but should be done in pratice:\n",
    "- Hyperparameter search\n",
    "- Cross validation\n",
    "- Reporting multiple seeds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
