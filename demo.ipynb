{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Model Selection\n",
    "\n",
    "Think about you use case an pick a model.\n",
    "\n",
    "**Questions to consider:**\n",
    "\n",
    "- Do you want to run it on your own hardware?\n",
    "- Do you have funds to use for a proprietary model?\n",
    "- Do you have strong programming skills?\n",
    "\n",
    "**Example use-case:**\n",
    "\n",
    "- Predicting vote choice in the US presidential election based on ANES survey answer.\n",
    "\n",
    "**Resources**\n",
    "\n",
    "- The popular benchmark **Chatbot Arena** _(Chiang et al., 2024)_  prives a live updated elo score based on crowdsourced pariwise comparisons of their responses.\n",
    "    - Trade-off Plot: https://lmarena.ai/price\n",
    "- The **Artificial Analysis** platform provides independent analysis of AI models and API providers. The main metrics are 'intelligence', speed, and price. \n",
    "    - https://artificialanalysis.ai/\n",
    "- The **Open LLM Leaderboard** on Hugging Face runs mutiple well established benchmarks on for open-source models hosted on the site. It is most usefull to compare smaller open-source LLMs that are not featured on Chatbot Arena or compare different fine-tunes of models. \n",
    "    - https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "Feel free to make notes here, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional colab/kaggle setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_dependencies():\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "      print(\"CUDA is not available. \\nPick a GPU before running this notebook. \\nGo to 'Runtime' -> 'Change runtime type' to do this. (Colab)\")\n",
    "      return \n",
    "    %pip install numpy==1.* # ligtheval is not compatible with 2.0\n",
    "    %pip install lighteval\n",
    "    %pip install transformers\n",
    "    %pip install datasets\n",
    "    %pip install peft\n",
    "    %pip install bitsandbytes\n",
    "    %pip install evaluate\n",
    "    %pip install wandb\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def is_running_in_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "def is_running_in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "if is_running_in_colab() or is_running_in_kaggle():\n",
    "    print(\"Running on Colab/Kaggle\")\n",
    "    install_dependencies()\n",
    "else:\n",
    "    print(\"Not running in Colab/Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "seed = 24 # Please set your own favorite seed!\n",
    "transformers.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation\n",
    "We use 2016 American National Election Studies survey data. Specifically, the subset of data Argyle et al. (2022) used in study 2 (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JPV20K)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "!curl -L -o 2016_anes_argyle.pkl https://github.com/tobihol/survai-finetuning/raw/main/2016_anes_argyle.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survey = pd.read_pickle(\"2016_anes_argyle.pkl\")\n",
    "df_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null values and data types\n",
    "df_survey.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"race\",\n",
    "    \"discuss_politics\",\n",
    "    \"ideology\",\n",
    "    \"party\",\n",
    "    \"church_goer\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"political_interest\",\n",
    "    \"patriotism\",\n",
    "    \"state\",\n",
    "]\n",
    "label = \"ground_truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tread missing values as a category \n",
    "df_survey_processed = (\n",
    "    df_survey\n",
    "    .astype({\"age\": str})\n",
    "    .fillna(\"missing\")\n",
    ")\n",
    "df_survey_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split\n",
    "\n",
    "Any manipulation of the training data should be done in the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_survey_processed, test_size=0.2, random_state=seed)\n",
    "\n",
    "# we can modify the training data here to do different experiments\n",
    "# for example excluding republican voters\n",
    "# leans_republican = df_train[\"party\"].apply(lambda x: \"Republican\" in x)\n",
    "# df_train = df_train[~leans_republican]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(df_test, preserve_index=False),\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Design\n",
    "\n",
    "We will use an instruction-tuned model and will therefore define an instruction prompt here. Having distinct `user` and `assistant` text. Where the `user` prompt includes all conditioning of the model and the `assistant` text is the expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"Please perform a classification task. \"\n",
    "    \"Given the 2016 survey answers from the American National Election Studies, \"\n",
    "    \"return which candiate the person voted for. \"\n",
    "    \"Return a label from ['Trump', 'Clinton', 'Non-voter'] only without any other text.\\n\"\n",
    ")\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_map = {\n",
    "    \"race\": \"Race\",\n",
    "    \"discuss_politics\": \"Discusses politics\",\n",
    "    \"ideology\": \"Ideology\",\n",
    "    \"party\": \"Party\",\n",
    "    \"church_goer\": \"Church\",\n",
    "    \"age\": \"Age\",\n",
    "    \"gender\": \"Gender\",\n",
    "    \"political_interest\": \"Political interest\",\n",
    "    \"patriotism\": \"American Flag\",\n",
    "    \"state\": \"State\",\n",
    "    \"ground_truth\": \"Vote\",\n",
    "}\n",
    "\n",
    "def map_to_prompt(row):\n",
    "    user_prompt = instruction\n",
    "    user_prompt += \"\\n\".join([f\"{column_name_map[k]}: {v}\" for k, v in row.items() if k != label])\n",
    "    assistant_prompt = row[label]\n",
    "    return {\n",
    "        \"text\": user_prompt, \n",
    "        \"label\": assistant_prompt,\n",
    "        }\n",
    "\n",
    "map_to_prompt(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_to_prompt(dataset['train'][0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_llm = dataset.map(map_to_prompt).remove_columns(features+[label])\n",
    "dataset_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "### Which model should I fine-tune? \n",
    "\n",
    "https://huggingface.co/models\n",
    "\n",
    "State-of-the-Art open-source model: **Llama 3 model family** *(Dubey et al., 2024)*\n",
    "- Best performance, useful for testing the best possible performance to data\n",
    "- First-party instruction-tuned models available\n",
    "\n",
    "Research model: **Pythia model family** *(Biderman et al., 2023)*\n",
    "- Openly available training data\n",
    "- Multiple smaller model sizes available\n",
    "- Enables testing your finetuning pipeline more efficiently\n",
    "- Enables comparing the effects of model size on performance\n",
    "- Easy to test for data contamination\n",
    "- Drawback: May not give a good representation of what is possible with SOTA models\n",
    "\n",
    "\n",
    "\n",
    "### Which models currently perform best?\n",
    "Popular benchmarks:\n",
    "- https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n",
    "- https://lmarena.ai/\n",
    "- https://crfm.stanford.edu/helm/\n",
    "    - Imputation Benchmark: https://crfm.stanford.edu/helm/classic/latest/#/groups/entity_data_imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # revision=revision, # NOTE: revision should be set for an reproducible experiment \n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with model versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem I encounted during my pipeline implementations using: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this if you don't have a huggingface account\n",
    "if False:\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello world\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Hello\"},\n",
    "    ]\n",
    "\n",
    "    tokenizer_mistral_old = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        revision=\"41b61a33a2483885c981aa79e0df6b32407ed873\",\n",
    "    )\n",
    "\n",
    "    untokenized_output_mistral_old = tokenizer_mistral_old.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    print(f\"Untokenized output: {untokenized_output_mistral_old}\")\n",
    "\n",
    "    tokenized_output_mistral_old = tokenizer_mistral_old.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=True,\n",
    "    )\n",
    "    print(f\"Tokenized output: {tokenized_output_mistral_old}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell Output\n",
    "\n",
    "Untokenized output: \\<s\\>[INST] Hello world [/INST]Hello\\</s\\>\n",
    "\n",
    "Tokenized output: [1, 733, 16289, 28793, 22557, 1526, 733, 28748, 16289, 28793, 16230, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this if you don't have a huggingface account\n",
    "if False:\n",
    "    tokenizer_mistral_new = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\", revision=\"main\"\n",
    "    )\n",
    "\n",
    "    untokenized_output_mistral_new = tokenizer_mistral_new.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    print(f\"Untokenized output: {untokenized_output_mistral_new}\")\n",
    "\n",
    "    tokenized_output_mistral_new = tokenizer_mistral_new.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=True,\n",
    "    )\n",
    "    print(f\"Tokenized output: {tokenized_output_mistral_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell Output\n",
    "\n",
    "Untokenized output: \\<s\\> [INST] Hello world [/INST] Hello\\</s\\>\n",
    "\n",
    "Tokenized output: [1, 733, 16289, 28793, 22557, 1526, 733, 28748, 16289, 28793, 22557, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> The token for the `Hello` answer of the assitant is different!\n",
    "\n",
    "Not only dependency versions should be reported, but also the model version! As even small changes in the tokenizer can cause major changes in the output and make a finding not reproducable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_tokenize_function(examples):\n",
    "    prompt = f\"{examples['text']} \\nVote: {examples['label']} {tokenizer.eos_token}\"\n",
    "    return tokenizer(prompt)\n",
    "\n",
    "\n",
    "def instruct_tokenize_function(examples):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": examples[\"text\"],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": examples[\"label\"],\n",
    "        },\n",
    "    ]\n",
    "    inputs_ids = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    attention_mask = np.ones_like(inputs_ids)\n",
    "    return {\n",
    "        \"input_ids\": inputs_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "tokenized_dataset_llm = dataset_llm.map(instruct_tokenize_function).remove_columns(\n",
    "    [\"text\", \"label\"]\n",
    ")\n",
    "tokenized_dataset_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenized_dataset_llm[\"train\"][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "Quantization reduces the memory required to store the model (Dettmers et al., 2022). Typically a model is stored in 16-bit precision, therefore for a 70B parameter model:\n",
    "\n",
    "$$\\frac{16 \\text{ bits}}{8 \\text{ bits/byte}} \\times 70 \\times 10^9 \\text{ parameters} = 140 \\text{ GB of VRAM}$$\n",
    "\n",
    "With 4-bit quantization, all parameters are stored in 4-bit precision, reducing the memory requirement to:\n",
    "\n",
    "$$\\frac{4 \\text{ bits}}{8 \\text{ bits/byte}} \\times 70 \\times 10^9 \\text{ parameters} = 35 \\text{ GB of VRAM}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model in 4bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "if getattr(model.config, \"pad_token_id\") is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "Low-Rank Adapters (LoRA) are a parameter efficient fine-tuning method (Hu et al., 2021). Instead of finetuning all model weights, LoRA finetunes the weights of the adapter layers only. This requires less memory and allows for faster finetuning.\n",
    "\n",
    "![LoRA Diagram](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Answer Extraction Problem\n",
    "\n",
    "<!-- - https://arxiv.org/pdf/2307.09702, https://github.com/dottxt-ai/outlines -->\n",
    "Different models for answer extraction:\n",
    "- https://blog.eleuther.ai/multiple-choice-normalization/\n",
    "- https://github.com/huggingface/lighteval\n",
    "\n",
    "Problem 1: How many tokens are needed to answer the question:\n",
    "- One-token solutions:\n",
    "    - less compute intensive\n",
    "    - do not require normalization\n",
    "    - only works if all first tokens are distinct\n",
    "- Multi-token solutions:\n",
    "    - more compute intensive (multiplied by number of labels)\n",
    "    - might require normalization\n",
    "    - does not require all first tokens to be distinct\n",
    "    \n",
    "Problem 2: How to evaluate multi token extraction (see code below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lighteval.metrics.metrics_sample import LoglikelihoodAcc\n",
    "from lighteval.metrics.normalizations import (\n",
    "    LogProbCharNorm,\n",
    "    # LogProbTokenNorm,\n",
    "    # LogProbPMINorm,\n",
    ")\n",
    "from lighteval.tasks.requests import Doc\n",
    "import numpy as np\n",
    "\n",
    "choices = [\"Trump\", \"Clinton\", \"Non-voter\"]\n",
    "log_prob_predictions = np.log([0.34, 0.33, 0.32])\n",
    "correct_choice = \"Non-voter\"\n",
    "\n",
    "doc = Doc(query=\"...\", choices=choices, gold_index=[choices.index(correct_choice)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_without_normalization = LoglikelihoodAcc(\n",
    "    # LogProbCharNorm(ignore_first_space=False),\n",
    ").compute(\n",
    "    gold_ixs=doc.gold_index,\n",
    "    choices_logprob=log_prob_predictions,\n",
    "    unconditioned_logprob=None,\n",
    "    choices_tokens=None,\n",
    "    formatted_doc=doc,\n",
    ")\n",
    "print(f\"Accuracy score without normalization: {acc_without_normalization}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_with_normalization = LoglikelihoodAcc(\n",
    "    LogProbCharNorm(ignore_first_space=False),\n",
    ").compute(\n",
    "    gold_ixs=doc.gold_index,\n",
    "    choices_logprob=log_prob_predictions,\n",
    "    unconditioned_logprob=None,\n",
    "    choices_tokens=None,\n",
    "    formatted_doc=doc,\n",
    ")\n",
    "print(f\"Accuracy score with normalization: {acc_with_normalization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from functools import partial\n",
    "\n",
    "hf_metrics = [\n",
    "    evaluate.load(\"accuracy\"),\n",
    "    # additional metrics can be added here\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def instruct_tokenization(\n",
    "    data: DatasetDict,\n",
    "    tokenizer: AutoTokenizer,\n",
    ") -> Tuple[DatasetDict, Dataset]:\n",
    "    def tokenize_function(examples, is_inference=False):\n",
    "        prompt = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"text\"]},\n",
    "        ]\n",
    "        if not is_inference:\n",
    "            prompt.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": examples[\"label\"],\n",
    "                }\n",
    "            )\n",
    "        inputs_ids = tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            add_generation_prompt=is_inference,\n",
    "        )\n",
    "        attention_mask = np.ones_like(inputs_ids)\n",
    "        return {\n",
    "            \"input_ids\": inputs_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "    column_names = list(data.column_names.values())[0]\n",
    "    training_data = data.map(tokenize_function, remove_columns=column_names)\n",
    "    from functools import partial\n",
    "\n",
    "    inference_data = data.map(\n",
    "        partial(tokenize_function, is_inference=True), remove_columns=column_names\n",
    "    )\n",
    "\n",
    "    answer_tokens = list(\n",
    "        {\n",
    "            training_ids[len(inference_ids)]\n",
    "            for inference_ids, training_ids in zip(\n",
    "                inference_data[\"train\"][\"input_ids\"]\n",
    "                + inference_data[\"test\"][\"input_ids\"],\n",
    "                training_data[\"train\"][\"input_ids\"]\n",
    "                + training_data[\"test\"][\"input_ids\"],\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    assert len(answer_tokens) == len(\n",
    "        set(data[\"test\"][\"label\"] + data[\"train\"][\"label\"])\n",
    "    )\n",
    "\n",
    "    return training_data, inference_data, answer_tokens\n",
    "\n",
    "\n",
    "training_data, inference_data, answer_tokens = instruct_tokenization(\n",
    "    dataset_llm, tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    logits = logits[:, :, answer_tokens].argmax(dim=-1)\n",
    "\n",
    "    return torch.tensor(\n",
    "        answer_tokens,\n",
    "        device=\"cuda\",\n",
    "    )[logits]\n",
    "\n",
    "# we use the first answer tokens of the assitant as the ground truth\n",
    "ground_truth = [training_ids[len(inference_ids)] for training_ids, inference_ids in zip(training_data[\"test\"][\"input_ids\"], inference_data[\"test\"][\"input_ids\"])]\n",
    "\n",
    "def compute_metrics_inference(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # NOTE: we assume that the eval dataset does not get shuffled, and we can therefor directly compare with the ground truth\n",
    "    y_true = ground_truth\n",
    "    # we calculate the prediction by taking the last non-padding token\n",
    "    y_pred = [[token for token in row if token != -100][-1] for row in preds]\n",
    "\n",
    "    results = {}\n",
    "    for metric in hf_metrics:\n",
    "        results |= metric.compute(predictions=y_pred, references=y_true)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "To run the training without wandb logging, set `wandb.init(mode='disabled')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "dataset_name = \"argyle_anes_2016\" # change the name here to your dataset\n",
    "run_name = f\"{model_id}_{dataset_name}_seed_{seed}_{now}\"\n",
    "\n",
    "# wandb.init(\n",
    "#     mode='disabled',\n",
    "# )\n",
    "wandb.init(\n",
    "    project=\"survai-finetuning\",\n",
    "    name=run_name,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=training_data[\"train\"],\n",
    "    eval_dataset=inference_data[\"test\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "\n",
    "        # train/eval settings\n",
    "        num_train_epochs=1, # NOTE you should run multiple epochs in practice\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=1 / 3,  # after each third of an epoch\n",
    "\n",
    "        # logging\n",
    "        logging_steps=10,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=run_name,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "        tokenizer, mlm=False\n",
    "    ),\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics_inference,\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fine-tuning results can be found live at: https://wandb.ai/tobihol/survai-finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic Non-responses Experiment\n",
    "\n",
    "The party affiliation is (obviously) a strong predictor of vote choice. In the Argyle et al. (2022) study, the GPT-3 mainly used the party affiliation and ideology of a person to predict the vote choice.\n",
    "\n",
    "In this experiment we remove Repulican voters from the train set. We therefore only train on democrats and independents and see if the model can still perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"party\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leans_republican = df_train[\"party\"].apply(lambda x: \"Republican\" in x)\n",
    "df_train[~leans_republican]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the notebook with modified training data split to do this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things we did not cover\n",
    "\n",
    "Some parts of the pipeline we did not do, because of time constraints, but should be done in pratice:\n",
    "- Hyperparameter search\n",
    "- Cross validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
